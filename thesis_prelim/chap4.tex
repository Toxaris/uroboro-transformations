% !TEX root = main.tex
\chapter{Automatic de- and refunctionalization}
\label{ch:derefunc}

\begin{figure}
\begin{subfigure}{\textwidth}
\begin{align*}
\langle prg \rangle^d & = ~&& \{ && \textrm{\textbf{data }} \sigma \textrm{\textbf{ where }} \span\span\span\span \\
& && && && \{ \langle fun \rangle^d (\tau_1, ..., \tau_n): \sigma ~ | ~ `` fun(\tau_1, ..., \tau_n): \tau " \in prg \} \\
& && | && `` \textrm{\textbf{codata }} \sigma ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} \langle des \rangle^d (\sigma, \tau_1, ..., \tau_n) \textrm{\textbf{ where}} \span\span\span\span \\
& && && && \{ \langle des \rangle^d (\langle fun \rangle^d (\overline{x}), \overline{y}) = \langle t \rangle^d ~ | ~ `` fun(\overline{x}).des(\overline{y}) = t " \in prg \} \\
& && | && `` \sigma.des(\tau_1, ..., \tau_n) " \in prg \} \span\span\span\span
\end{align*}
\caption{Defunctionalization for the Codata Fragment}
\end{subfigure}

\begin{subfigure}{\textwidth}
\begin{align*}
\langle prg \rangle^r & = ~&& \{ && \textrm{\textbf{codata }} \sigma \textrm{\textbf{ where }} \span\span\span\span \\
& && && && \{ \sigma.\langle fun \rangle^d (\tau_1, ..., \tau_n): \tau ~ | ~ `` fun(\sigma, \tau_1, ..., \tau_n): \tau " \in prg \} \\
& && | && `` \textrm{\textbf{data }} \sigma ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} \langle con \rangle^r (\tau_1, ..., \tau_n): \tau \textrm{\textbf{ where}} \span\span\span\span \\
& && && && \{ \langle con \rangle^r (\langle fun \rangle^d (\overline{x}), \overline{y}) = \langle t \rangle^d ~ | ~ `` fun(con(\overline{x}), \overline{y}) = t " \in prg \} \\
& && | && `` con(\tau_1, ..., \tau_n): \tau " \in prg \} \span\span\span\span
\end{align*}
\caption{Refunctionalization for the Data Fragment}
\end{subfigure}
\caption{Rendel et al.'s de- and refunctionalization}
\label{fig:ch4rendelderefunc}
\end{figure}

\begin{figure}
\begin{align*}
& \textsf{Unnest}_i: \mathbf{U}_{cc} \to \mathbf{U}_{cc}[fdef_{un,i}] \text{ for } i \in \{d,r\} \\
& \textsf{CoreDefunc}: \mathbf{U}_{cc}[fdef_{un,d}] \to \mathbf{U}^{defunc}_{cc} \\
& \textsf{CoreRefunc}: \mathbf{U}_{cc}[fdef_{un,r}] \to \mathbf{U}^{refunc}_{cc}
\end{align*}
\caption{Domains and ranges for the algorithms of ch.4}
\label{fig:ch4domrng}
\end{figure}

Defunctionalization and refunctionalization have already been outlined in the introduction. In this chapter, we build upon the works of \citet{rendel15automatic} and \citet{setzer14unnesting} to automatize defunctionalization and refunctionalization for Uroboro. 

We present an algorithm that defunctionalizes arbitrary Uroboro programs with copattern coverage, and an algorithm that refunctionalizes such programs. Both algorithms are made up of two phases, in order:
\begin{enumerate}
\item Unnesting, and
\item core de-/refunctionalization.
\end{enumerate}

In short, the entire process can be described as follows. By unnesting them as far as necessary, the preprocessing phase brings the lhss of the program to the form accepted by the core de-/refunctionalization, which is essentially the two-way transformation of Rendel et al.; their transformations are repeated in \autoref{fig:ch4rendelderefunc} for the convenience of the reader.

The unnesting is done by a parameterized algorithm $\textsf{Unnest}_i$, where the parameter $i$ can be either $d$ or $r$, standing for defunctionalization and refunctionalization, respectively. The parameter determines when the algorithm may stop, such that its results are legal inputs for de- or refunctionalization. The unnesting algorithm is essentially the copattern unnesting algorithm of \citet{setzer14unnesting}, adapted to Uroboro.

In \autoref{fig:ch4domrng}, we give the intended domain and range for each of the algorithms, all of which are specific fragments of Uroboro. We call the set of all Uroboro programs with copattern coverage $\mathbf{U}_{cc}$; the other fragments are defined below.

\begin{definition}[Unnested Fragments for \textsf{CoreDefunc} and \textsf{CoreRefunc}]
The fragments $\mathbf{U}_{cc}[fdef_{un,d}]$, $\mathbf{U}_{cc}[fdef_{un,r}]$, for lhss unnested as far as necessary for \textsf{CoreDefunc} and \textsf{CoreRefunc}, respectively, are induced by the function definition fragments $fdef_{un,d}$ and $fdef_{un,r}$, respectively, both defined below using EBNF rules. For these, the basic EBNF rules from the definition of the syntax of Uroboro are reused.
\begin{align*}
fdef_{un,d} &::= eqn_{nd}^* ~ | ~ eqn_d^* \\
eqn_{nd} &::= fun(x^*).des(y^*) = t \\
eqn_d &::= fun(p^*) = t \\
\end{align*}
\begin{align*}
fdef_{un,r} &::= eqn_{nr}^* ~ | ~ eqn_r^* \\
eqn_r &::= fun(x^*).des(y^*)^* = t \\
eqn_{nr} &::= fun(x^*, con(y^*), z^*) = t \\
\end{align*}
\end{definition}

\begin{definition}[Defunctionalized fragment]
A Uroboro program is in the defunctionalized fragment $\mathbf{U}^{defunc}_{cc}$ if and only if it has copattern coverage and contains no codata type definitions.
\end{definition}

\begin{definition}[Refunctionalized fragment]
A well-typed Uroboro program is in the refunctionalized fragment $\mathbf{U}^{refunc}_{cc}$ if and only if it has copattern coverage and contains no data type definitions.
\end{definition}

For each algorithm it will be shown, in its respective section, that it actually has the range specified above. In total, four important properties will be shown for each algorithm: Termination, correct range, preservation of well-typedness, and preservation of semantics in a (weak) bisimulation.

The rest of this chapter is organized as follows. The \hyperref[sec:unn]{first section} describes the unnesting algorithm and shows why the four properties hold. Sections \hyperref[sec:coredefunc]{2} and \hyperref[sec:corerefunc]{3} do the same for the core de- and refunctionalization, respectively.

\section{Unnesting}
\label{sec:unn}

We adapt the translation algorithm of \citet[Section 3.3]{setzer14unnesting} to Uroboro. Their algorithm translates equations step-by-step depending upon the derivation of the copattern coverage of their lhss. Our adaptation only has steps which concern patterns that have a counterpart in Uroboro\footnote{I.e., not those for applications, units, and pairs. In the adapted algorithm for Uroboro, the last two are merged into the steps for destructors and constructors, while application is irrelevant because Uroboro doesn't have function types.}. Each such translation step can be considered an extraction as defined in \autoref{ch:extr}.

\subsection{Simple patterns}

Before we start with the translation algorithm, we adapt and introduce, respectively, two notions of simplicity for patterns, the second of which corresponds to the intended range of the translation algorithm. We first adapt the definition of a simple pattern, as given by Setzer et al., to Uroboro. Call a copattern \textit{simple} if it is of one of the three forms $fun(\overline{x})$, $fun(\overline{x}).des(\overline{y})$, or $fun(\overline{x}, con(\overline{y}), \overline{z})$.

Next, we generalize this definition to define \textit{sufficiently simple patterns} for a purpose, which is either de- or refunctionalization. Call a copattern \textit{sufficiently simple for defunctionalization} if it is of one of the two forms $fun(\overline{x}).des(\overline{y})$, $fun(\overline{p})$, and \textit{sufficiently simple for refunctionalization} if it is of one of the two forms $fun(\overline{x}).\overline{des(\overline{y})}$, $fun(\overline{x}, con(\overline{y}), \overline{z})$. We will also say that a copattern is sufficiently simple for $i$, for $i \in \{d,r\}$, and mean that it is sufficiently simple for defunctionalization or refunctionalization, respectively.

Those notions correspond directly to the forms of lhss allowed in the unnested fragments for \textsf{CoreDefunc} and \textsf{CoreRefunc}, respectively. It is clear that each simple copattern is also sufficiently simple for both de- and refunctionalization.

\subsection{Translation algorithm}
\label{ssec:unntransl}

\subsubsection{Example}

To illustrate what the algorithms $\textsf{Unnest}_d$ and $\textsf{Unnest}_r$ are doing, we first show what happens when either algorithm is used on the example program presented in \autoref{fig:ch2uroex}.

\textbf{Unnesting for defunc.} We begin with $\textsf{Unnest}_d$, and first consider the function definition for \texttt{oneElemArray}.

\begin{lstlisting}
function oneElemArray(Nat): Array where
  oneElemArray(n).get(Zero()) = n
  oneElemArray(n).get(Succ(m)) = Zero()
\end{lstlisting}

The goal is to make the lhss of the function definition sufficiently simple for defunctionalization. As both lhss have both destructors and constructors, this isn't the case yet. Unnesting works on the derivation tree for copattern coverage, thus we first need this derivation tree. By trying all possible derivation trees of depth two (maximal number of constructors plus number of destructors in any lhs), we get the following derivation tree.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ \texttt{oneElemArray(n)}$}
\RightLabel{C\textsubscript{Des}}
\UnaryInfC{$fun \lhd | ~ \texttt{oneElemArray(n).get(m)} $}
\RightLabel{C\textsubscript{Con}}
\UnaryInfC{$fun \lhd | ~ \texttt{oneElemArray(n).get(Zero())} ~ \texttt{oneElemArray(n).get(Succ(m))} $}
\end{prooftree}

The last step in the derivation is a variable split, i.e., introduction of constructor calls, at the single argument of the destructor \texttt{get}. Therefore, we simplify the lhss of the program by extracting these constructor calls into an auxiliary function definition, resulting in the following program.

\begin{lstlisting}
function oneElemArray(Nat): Array where
  oneElemArray(n).get(m) = aux(n, m)

function aux(Nat, Nat): Array where
  aux(n, Zero()) = n
  aux(n, Succ(m)) = Zero()
\end{lstlisting}

Now, \texttt{oneElemArray} has only one lhs, and this is sufficiently simple for defunctionalization since it has only one destructor and no constructors. The auxiliary function $aux$ also has only sufficiently simple lhss; in fact, this is the case for all auxiliary functions introduced by the unnesting. The remaining function definitions are sufficiently simple for unnesting, as well, thus we are done.

\textbf{Unnesting for refunc.} Next, we use $\textsf{Unnest}_r$ on the example. We first consider \texttt{oneElemArray}, and again use the coverage derivation tree from above. The same constructor extraction as with $\textsf{Unnest}_d$ is carried out; the resulting function definitions are also sufficiently simple for refunctionalization. Next, we turn to \texttt{isOne}: it's lhss are sufficiently simple for defunctionalization, but not for refunctionalization, since it contains nested constructor calls.

\begin{lstlisting}
function isOne(Nat): Nat where
  isOne(Zero()) = Zero()
  isOne(Succ(Zero())) = Succ(Zero())
  isOne(Succ(Succ(n))) = Zero()
\end{lstlisting}

We try all possible derivation trees of depth two, and get the following derivation tree.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ \texttt{isOne(n)}$}
\RightLabel{C\textsubscript{Con}}
\UnaryInfC{$fun \lhd | ~ \texttt{isOne(Zero())} ~ \texttt{isOne(Succ(n))} $}
\RightLabel{C\textsubscript{Con}}
\UnaryInfC{$fun \lhd | ~ \texttt{isOne(Zero())} ~ \texttt{isOne(Succ(Zero()))} ~ \texttt{isOne(Succ(Succ(n)))} $}
\end{prooftree}

The last step in the derivation is a variable split; we ``undo'' this by extracting the respective constructor calls into an auxiliary function. In the terminology developed in \autoref{ch:extr}, we target the last two lhss in the function definition for constructor extraction; the position to be targeted is the (only) argument position of the constructor call that is the first argument of the function call in both lhss.

\begin{lstlisting}
function isOne(Nat): Nat where
  isOne(Zero()) = Zero()
  isOne(Succ(n)) = aux2(n)

function aux2(Nat): Nat where
  aux2(Zero()) = Succ(Zero())
  aux2(Succ(n)) = Zero()
\end{lstlisting}

Now, all lhss of the program are sufficiently simple for refunctionalization, thus we are done.

\subsubsection{Formal description}

Now, we semi-formally describe the algorithm for $\textsf{Unnest}_i$, adapted from \citet[Section 3.3]{setzer14unnesting} to Uroboro, as lined out above. We assume that all copatterns in the program to be transformed comply with some variable naming schema as described in \autoref{ssec:conextr}; if this is not yet the case, simply rename the variables according to the schema before applying $\textsf{Unnest}_i$.

\begin{algorithm}[$\textsf{Unnest}_i$]

The algorithm is applied to an individual function definition; to transform the entire program, simply apply it to each of them in arbitrary order. Like \citeauthor{setzer14unnesting}, we consider the last step in the derivation of the copattern coverage for the lhss of a function definition. Just like them, we are not interested in the case where the last step is C\textsubscript{Head}, since that means the lhs is already simple, and we terminate. When the last step isn't C\textsubscript{Head}, but the lhss are all sufficiently simple for $i$ nonetheless, we also terminate.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q)$}
\RightLabel{\textbf{C}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q_i)_{i \in I}$}
\end{prooftree}

Also like \citeauthor{setzer14unnesting}, we then introduce a fresh auxiliary function and translate the program depending on \textbf{C}. As stated before, these translations are slightly adapted to fit the framework of Uroboro, such that two remain, one for destructors and one for constructors. More importantly, both of those are in fact extractions as defined in \autoref{ch:extr}, namely destructor and constructor extraction. 

\begin{enumerate}
\item \textbf{C} is C\textsubscript{Des}, and the last derivation step looks as follows.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q^\tau)$}
\RightLabel{C\textsubscript{Des}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q.des(\overline{x^{des}}))_{des \in Dess_\tau}$}
\end{prooftree}

Apply extraction \textsf{ExtractDes} targeting the equations with lhss $q.des(\overline{x^{des}})$, for all $des \in Dess_\tau$, to the function definition for $fun$. From this we obtain the changed function definition $def'_{fun}$ for $fun$ and the auxiliary function definition. The lhss of $def'_{fun}$ are
\[
Q \cup \{ \textsf{get}(q.des(\overline{x^{des}})) ~ | ~ des \in Dess_\tau \} = Q \cup \{q\},
\]
where the pair of \textsf{get} and \textsf{putback} is the lens underlying \textsf{ExtractDes}. This set of lhss is the same as that in $fun \lhd | ~ Q ~ (q^\tau)$ from the derivation step above. The lhss of the auxiliary function definition are
\begin{multline*}
\{ q_{\zeta_r} ~ | ~ r \in T \} = \{ \textsf{putback}(\langle \textsf{get}(q_r) \rangle^{aux}, q_r) ~ | ~ r \in T \} \\
= \{ \sigma^{q_r}_\pi(aux(\langle q \rangle^{vars})) ~ | ~ r \in T \} = \{ aux(\langle q \rangle^{vars}).des(\overline{x^{des}}) ~ | ~ des \in Dess_\tau \} \\
= \{ aux(\langle q \rangle^{vars}).des(\overline{x^{des}}) ~ | ~ des \in Dess_\tau \},
\end{multline*}
where $\zeta$ is the function in the triple that is \textsf{ExtractCon}(p). It is clear that each $q_{\zeta_r}$ is simple and that $\{ q_{\zeta_r} ~ | ~ r \in T \}$ covers $aux$:
\begin{prooftree}
\AxiomC{}
\RightLabel{C\textsubscript{Head}}
\UnaryInfC{$aux \lhd | ~ aux(\langle q \rangle^{vars})$}
\RightLabel{C\textsubscript{ResSplit}}
\UnaryInfC{$aux \lhd | ~ (q_{\zeta_r})_{r \in T}$}
\end{prooftree}

\item \textbf{C} is C\textsubscript{Con}, and the last derivation step looks as follows. 

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q(x^\tau))$}
\RightLabel{C\textsubscript{Con}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q[x \mapsto con(\overline{y^{con}})])_{con \in Cons_\tau}$}
\end{prooftree}

Let $\ell$ be the position of $x$ in $q$. Since we assume that $q$ complies with some variable naming schema as described in \autoref{ssec:conextr}, the name of $x$ is $\textsf{name}(\ell)$, where \textsf{name} is the naming function according to the schema. Apply extraction $\textsf{ExtractCon}(\ell)$ targeting the equations with lhss $q[\textsf{name}(\ell) \mapsto con(\overline{y^{con}})]$, for all $con \in Cons_\tau$ to the original function definition for $fun$. From this we obtain the changed function definition $def'_{fun}$ for $fun$ and the auxiliary function definition. The lhss of $def'_{fun}$ are
\begin{multline*}
Q \cup \{ \textsf{get}(q[\textsf{name}(\ell) \mapsto con(\overline{y^{con}})]) ~ | ~ con \in Cons_\tau \} \\
= Q \cup \{ q[\textsf{name}(\ell) \mapsto con(\overline{y^{con}})][con(\overline{y^{con}}) \mapsto \textsf{name}(\ell)]_\ell \} \\
= Q \cup \{q\},
\end{multline*}
where the pair of \textsf{get} and \textsf{putback} is the lens underlying $\textsf{ExtractCon}(\ell)$. This set of lhss is the same as that in $fun \lhd | ~ Q ~ (q)$ from the derivation step above. The lhss of the auxiliary function definition are
\begin{multline*}
\{ q_{\zeta_r} ~ | ~ r \in T \} = \{ \textsf{putback}(\langle \textsf{get}(q_r) \rangle^{aux}, q_r) ~ | ~ r \in T \} \\
= \{ \sigma^{q_r}_\pi(aux(\langle q \rangle^{vars})) ~ | ~ r \in T \} \\
= \{ aux(\langle q \rangle^{vars})[\textsf{name}(\ell) \mapsto con(\overline{y^{con}})] ~ | ~ con \in Cons_\tau \}
\end{multline*}
where $\zeta$ is the function in the triple that is $\textsf{ExtractCon}(\ell)$. It is clear that each $q_{\zeta_r}$ is simple and that $\{ q_{\zeta_r} ~ | ~ r \in T \}$ covers $aux$:
\begin{prooftree}
\AxiomC{}
\RightLabel{C\textsubscript{Head}}
\UnaryInfC{$aux \lhd | ~ aux(\langle q \rangle^{vars})$}
\RightLabel{C\textsubscript{VarSplit}}
\UnaryInfC{$aux \lhd | ~ (q_{\zeta_r})_{r \in T}$}
\end{prooftree}
\end{enumerate}

As shown for each of the two cases, after the extractions copattern coverage still holds for the translated program, and we even know its derivation tree. Thus we can go back to the start and do further translations, if necessary.
\end{algorithm}

\subsubsection{Properties}

Next, we consider important properties of the translation algorithm.

\textbf{Termination.} To show that the algorithm terminates, we can basically repeat the argument of \citet[Section 3.3]{setzer14unnesting} Each translation results in a changed function definition $def'_{fun}$ for $fun$ and a new function definition $def_{aux}$ for the auxiliary function. The lhss of $def'_{fun}$ are $Q \cup \{q\}$, which means that they cover $fun$ in the same way $fun$ is covered via $fun \lhd | ~ Q ~ (q)$ in the derivation step above. This means that the translation reduces the depth of the derivation tree for the coverage of $fun$, thus eventually arriving at sufficiently simple patterns. The added auxiliary function definitions already have simple lhss, thus they are not translated, and eventually the algorithm terminates.

\textbf{Range.} The algorithm only stops when all lhss are sufficiently simple for $i$ and works on the coverage derivation trees, thus we have the desired range $\mathbf{U}_{cc}[fdef_{un,i}]$ if it terminates in all cases, which it does, as shown above.

\textbf{Preservation of well-typedness.} The only changes made to the program are a series of extractions (each is either constructor or destructor extraction). All extractions preserve the well-typedness of a program, therefore well-typedness is also preserved by $\texttt{Unnest}_i$.

\textbf{Bisimulation.} Because copattern coverage holds for the translated program, by \autoref{fac:ccnooverlap}, the translated program has no overlapping lhss. Therefore, by the propositions~\ref{eq:bisim1} and~\ref{eq:bisim2}, we know that the translation preserves the semantics of the program in the kind of weak bisimulation described in \autoref{sec:extrbis}.

\section{Core defunctionalization}
\label{sec:coredefunc}

The core defunctionalization \textsf{CoreDefunc} applies the defunctionalization of \citet{rendel15automatic} for the Codata Fragment to the parts of the program that are in this fragment, and leaves the rest, which is already in a defunctionalized form after \textsf{Unnest}, virtually unchanged. That is, the only change to these parts affects the right-hand sides of equations, which are transformed to account for the changed syntactic domains of the symbols.

\subsection{Example}

\begin{figure}
\begin{lstlisting}

data Nat where
  Zero(): Nat
  Succ(Nat): Nat

function add(Nat, Nat): Nat where
  add(Zero(), n) = n
  add(Succ(m), n) = Succ(add(m, n))

function multiply(Nat, Nat): Nat where
  multiply(Zero(), n) = Zero()
  multiply(Succ(m), n) = add(multiply(m, n), n)

codata Array where
  Array.get(Nat): Nat

function oneElemArray(Nat): Array where
  oneElemArray(n).get(m) = aux(n, m)

function aux(Nat, Nat): Array where
  aux(n, Zero()) = n
  aux(n, Succ(m)) = Zero()

function isOne(Nat): Nat where
  isOne(Zero()) = Zero()
  isOne(Succ(Zero())) = Succ(Zero())
  isOne(Succ(Succ(n))) = Zero()

\end{lstlisting}
\caption{Uroboro sample before core defunctionalization}
\label{fig:ch4urosamplepredefunc}
\end{figure}

\begin{figure}
\begin{lstlisting}

data Nat where
  Zero(): Nat
  Succ(Nat): Nat

function add(Nat, Nat): Nat where
  add(Zero(), n) = n
  add(Succ(m), n) = Succ(add(m, n))

function multiply(Nat, Nat): Nat where
  multiply(Zero(), n) = Zero()
  multiply(Succ(m), n) = add(multiply(m, n), n)

data Array where
  oneElemArray(Nat): Array

function get(Array, Nat): Array where
  get(oneElemArray(n), m) = aux(n, m)

function aux(Nat, Nat): Array where
  aux(n, Zero()) = n
  aux(n, Succ(m)) = Zero()

function isOne(Nat): Nat where
  isOne(Zero()) = Zero()
  isOne(Succ(Zero())) = Succ(Zero())
  isOne(Succ(Succ(n))) = Zero()

\end{lstlisting}
\caption{Uroboro sample after core defunctionalization}
\label{fig:ch4urosamplepostdefunc}
\end{figure}

Before we formally describe the defunctionalization algorithm, we illustrate it with an example, which we continue from that for $\textsf{Unnest}_d$. In \autoref{fig:ch4urosamplepredefunc}, we show the result of $\textsf{Unnest}_d$ applied to this example; it is sufficiently simple for our core defunctionalization.

All function definitions except for \texttt{oneElemArray} are already in a defunctionalized form, in the sense that they have no destructors in their lhss. All that \textsf{CoreDefunc} does with this program is to turn the function \texttt{oneElemArray} into a constructor for \texttt{Array}, which is now a data type replacing the original codata type, and to turn the destructor \texttt{Array.get} into a function that dispatches upon an \texttt{Array} argument. The result, as shown in \autoref{fig:ch4urosamplepostdefunc}, is free of codata types and copattern matching, as desired.

\subsection{Formal definition}

\begin{figure}
\begin{align*}
\langle x \rangle^d = x \\
\langle s.des(t_1, ..., t_n) \rangle^d = \langle des \rangle^d (\langle s \rangle^d, \langle t_1 \rangle^d, ..., \langle t_n \rangle^d) \\
\langle fun(t_1, ..., t_n) \rangle^d = fun(\langle t_1 \rangle^d, ..., \langle t_n \rangle^d), \text{ if } fun \in F^{prg}_d \\
\langle fun(t_1, ..., t_n) \rangle^d = \langle fun \rangle^d (\langle t_1 \rangle^d, ..., \langle t_n \rangle^d), \text{ if } fun \not\in F^{prg}_d \\
\langle con(t_1, ..., t_n) \rangle^d = con(\langle t_1 \rangle^d, ..., \langle t_n \rangle^d)
\end{align*}
For a symbol, $\langle \cdot \rangle^d$ means that the syntactic domain is changed: $\langle des \rangle^d$ is a function symbol, and for $fun \not\in F^{prg}_d$, $\langle fun \rangle^d$ is a constructor symbol. The transformation for symbols must be invertible, but the name of the new symbol is otherwise arbitrary.
\caption{Defunctionalization of terms}
\label{fig:termdefunc}
\end{figure}

First, we define the class of function definitions of a program $prg$ which are already defunctionalized. A function definition is already defunctionalized when (a) it has at least one equation and all of its equations have lhss without destructors or (b) it has no equations and the function's return type is a positive (data) type. Function definitions of kind (b) are those for which copattern coverage is derived by a variable split for a variable with an empty data type. Note that there is some degree of freedom in the definition of already defunctionalized function definitions: One might extend the class of already defunctionalized function definitions with those empty function definitions which have negative return type but where at least one argument type is an empty data type. We define the class of already defunctionalized functions $F^{prg}_d$ as those which have already defunctionalized function definitions.

Using this definition, we define defunctionalization $\langle \cdot \rangle^d$ of terms in \autoref{fig:termdefunc}; it depends upon $F^{prg}_d$ and therefore on the program $prg$. This transformation only changes symbols, but leaves the structure of the terms unchanged (disregarding the syntactic sugar for destructor calls). Especially, this means that $\langle q[\sigma] \rangle^d = \langle q \rangle^d [\langle \cdot \rangle^d \circ \sigma]$ for any copattern $q$ and substitution $\sigma$. A property that is important for the definition of \textsf{CoreDefunc} below is that, for any copattern $q$ that is allowed as a lhs in the domain of \textsf{CoreDefunc}, $\langle q \rangle^d$ is also a copattern. This can be easily shown by induction on the structure of $q$.

Next, we define \textsf{CoreDefunc}. Like \citet{rendel15automatic} do for their de- and refunctionalization, we use set-builder notation.

\begin{algorithm}[\textsf{CoreDefunc}]

\begin{align*}
\langle prg \rangle^{\textsf{CoreDefunc}} & = ~&& \{ && \textrm{\textbf{data }} \sigma \textrm{\textbf{ where }} \span\span\span\span \\
& && && && \{ \langle fun \rangle^d (\tau_1, ..., \tau_n): \sigma \\
& && && && | ~ `` fun(\tau_1, ..., \tau_n): \tau " \in prg \text{ s.t. } fun \not\in F^{prg}_d \} \\
& && | && `` \textrm{\textbf{codata }} \sigma ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} \langle des \rangle^d (\sigma, \tau_1, ..., \tau_n) \textrm{\textbf{ where}} \span\span\span\span \\
& && && && \{ \langle des \rangle^d (\langle fun \rangle^d (\overline{x}), \overline{y}) = \langle t \rangle^d ~ | ~ `` fun(\overline{x}).des(\overline{y}) = t " \in prg \} \\
& && | && `` \sigma.des(\tau_1, ..., \tau_n) " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{data }} ... ~ | ~ `` \textrm{\textbf{data }} ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} \span\span\span\span \\
& && && && \{ q = \langle t \rangle^d ~ | ~ ``q = t" \in eqns \} \span\span \\
& && | && `` \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} eqns " \text{ s.t. } fun \in F^{prg}_d \} \span\span\span\span
\end{align*}

\end{algorithm}

\subsection{Properties}

We consider important properties of the transformation.

\textbf{Termination.} In the set-builder notation we used to define the algorithm, it is clear how the resulting program is constructed by going through the syntax of the input program. That is, the set-builder definitions for each of the sets in the union can be implemented as loops over specific syntactic parts of the input program, like the codata signatures, the destructor signatures, or the equations. Some of the loops are nested, but each time the collection that is looped over is finite, thus the transformation algorithm terminates.

\textbf{Range.} It can be directly seen in the definition that the resulting program contains no codata type definitions. The copattern coverage of the resulting program is also easy to see. In the set-builder definition, the last set in the union contains function definitions taken over from the original program with only the right-hand sides of their equations changed (by $\langle \cdot \rangle^d$); since the left-hand sides stay the same, these function definitions again have copattern coverage. The function definitions from the second set in the union remain to be considered. For each of their equations, each of its left-hand sides is constructed (disregarding the receiver notation for destructors) from the original left-hand side by turning the destructor name into the corresponding function name and the function name into the corresponding constructor name. For the original function definitions, copattern coverage was derived by result splitting; consequently, copattern coverage for the new function definitions can be derived by variable splitting on the first variable.

\textbf{Preservation of well-typedness.} Let $prg$ be the program to be transformed; we assume $prg$ to be well-typed. This means that for each equation $eqn = `` q = t ''$ of $prg$ we have a derivation of $\Sigma_{prg} \vdash eqn \textrm{ ok}$, with $\Sigma_{prg}$ being the signatures of $prg$. We will show that each equation of $\langle prg \rangle^{\textsf{CoreDefunc}}$ is well-typed. In the definition of \textsf{CoreDefunc}, we can see that the signatures $\Sigma_{\langle prg \rangle^{\textsf{CoreDefunc}}}$ of $\langle prg \rangle^{\textsf{CoreDefunc}}$ are exactly the following: for each destructor signature in $\Sigma_{prg}$ an identical function signature, for some function signatures in $\Sigma_{prg}$ identical constructor signatures, and for the other function signatures in $\Sigma_{prg}$ the identical function signatures. From this, we have the following lemma.

\begin{lemma}
\label{lem:pwtdefunc}
When $\Sigma_{prg} \vdash `` q = t " \textrm{ ok}$, then $\Sigma_{\langle prg \rangle^{\textsf{CoreDefunc}}} \vdash `` \langle q \rangle^d = \langle t \rangle^d " \textrm{ ok}$.
\begin{proof}
By induction on the derivation of $\Sigma \vdash `` q = t " \textrm{ ok}$, using the similarity of $\Sigma_{prg}$ and $\Sigma_{\langle prg \rangle^{\textsf{CoreDefunc}}}$.
\end{proof}
\end{lemma}

In the definition of \textsf{CoreDefunc} we can see that each equation $eqn = `` q = t "$ in $prg$ corresponds to exactly one equation $`` \langle q \rangle^d = \langle t \rangle^d "$ of $\langle prg \rangle^{\textsf{CoreDefunc}}$. Combine this with \autoref{lem:pwtdefunc} to get $\Sigma_{\langle prg \rangle^{\textsf{CoreDefunc}}} \vdash `` q = t " \textrm{ ok}$ for each equation $eqn$ of $\langle prg \rangle^{\textsf{CoreDefunc}}$. Thus $\langle prg \rangle^{\textsf{CoreDefunc}}$ is well-typed whenever $prg$ is.

\textbf{Bisimulation.} For \textsf{CoreDefunc}, strong bisimulation holds. To show this, we first show a general result that an isomorphism between rule sets of two programs is also an isomorphism between the respective reduction relations of the programs. In the following, $\langle \cdot \rangle$ is some invertible program transformation when applied to programs. When applied to terms it stands for some invertible transformation of terms that only changes symbols, but leaves the structure of the terms unchanged.

\begin{lemma}[Rules isomorphism is reduction isomorphism]
\label{lem:iso}
For any program $prg$, when it is
\[
\forall s,t \in \textrm{Term}_{prg}. (s, t) \in \textrm{Rules}(prg) \iff (\langle s \rangle, \langle t \rangle) \in \textrm{Rules}(\langle prg \langle).
\]
, then it also holds that
\[
\forall s,t \in \textrm{Term}_{prg}. s \longrightarrow_{prg} t \iff \langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle.
\]
\end{lemma}

In order to prove this, we first show some auxiliary lemmas concerning the value judgement and evaluation contexts. Instead of always writing out
\[
\forall s,t \in \textrm{Term}_{prg}. (s, t) \in \textrm{Rules}(prg) \iff (\langle s \rangle, \langle t \rangle) \in \textrm{Rules}(prg),
\]
from now on we will simply say that $\langle \cdot \rangle$ is a rule isomorphism.

\begin{lemma}
\label{lem:ivj}
Let $\langle \cdot \rangle$ be a rule isomorphism. Then, for any program $prg$, it holds that
\[
\forall t \in \textrm{Term}_{prg}. prg \vdash_v t \implies \langle prg \rangle \vdash_v \langle t \rangle
\]
\begin{proof}
By induction on the structure of $t$. By the derivation of $prg \vdash_v t$ it is (a) $prg \vdash_v t_i$ for all immediate subterms $t_i$ of $t$ and (b) $t \neq^? q$ for all lhss $q$ of rules of $prg$. By the premiss we know that for all lhss $q'$ of rules of $\langle prg \rangle$, there is a lhs $q$ of $prg$ such that $q' = \langle q \rangle$; thus it is $\langle t \rangle \neq^? q'$ for all lhss $q'$ of rules of $\langle prg \rangle$. By the induction hypothesis, we know that $\langle prg \rangle \vdash_v \langle t_i \rangle$. Since the immediate subterms of $\langle t \rangle^d$ are the $\langle t_i \rangle$ for each $t_i$, we have the derivation for $\langle prg \rangle \vdash_v \langle t \rangle$.
\end{proof}
\end{lemma}

\begin{lemma}
\label{lem:iec}
Let $\langle \cdot \rangle$ be a rule isomorphism. Then, for any program $prg$, it holds that
\[
\forall \mathcal{E}. \mathcal{E} \in \textrm{EC}_{prg} \implies \langle \mathcal{E} \rangle \in \textrm{EC}_{\langle prg \rangle}
\]
\begin{proof}
By induction on the structure of $\mathcal{E}$.

When $\mathcal{E} = []$, then it is $\langle \mathcal{E} \rangle = []$, and this is clearly an evaluation context for $\langle prg \rangle$.

When $\mathcal{E} \neq []$, all immediate subterms $t_{left}$ of $\mathcal{E}$ to the left of the hole are values under judgement $prg \vdash_v$. The ``middle'' term that follows after all these is again an evaluation context $\mathcal{E}_0$ for $prg$. $\langle \mathcal{E} \rangle$ is $\mathcal{E}$ with each $t_{left}$ replaced by $\langle t_{left} \rangle$, the immediate subterms right of the hole replaced in the same way, and the ``middle'' term replaced by $\langle \mathcal{E}_0 \rangle$. By the induction hypothesis, $\langle \mathcal{E}_0 \rangle$ is an evaluation context for $\langle prg \rangle$ because $\mathcal{E}_0$ is one for $prg$. By \autoref{lem:ivj}, it is $\langle prg \rangle \vdash_v \langle t_{left} \rangle$ for all $t_{left}$ because $prg \vdash_v t_{left}$.
\end{proof}
\end{lemma}

Next, we show that one direction of \autoref{lem:iso} holds when restricted to contraction, from which the full \autoref{lem:iso} directly follows.

\begin{lemma}
\label{lem:isoc}
Let $\langle \cdot \rangle$ be a rule isomorphism. Then, for any program $prg$, it holds that
\[
\forall s,t \in \textrm{Term}_{prg}. s \mapsto_{prg} t \implies \langle s \rangle \mapsto_{\langle prg \rangle} \langle t \rangle.
\]
\begin{proof}
By the derivation of $s \mapsto_{prg} t$ there is a rule $r := (q_r, t_r) \in \textrm{Rules}(prg)$ and a substitution $\sigma$ such that $s = q_r[\sigma]$ and $t = t_r[\sigma]$ and the right-hand sides of $\sigma$ are values in $prg$. By the premiss we know that there is a rule $\langle r \rangle := (\langle q_r \rangle, \langle t_r \rangle) \in \textrm{Rules}(prg)$, and by \autoref{lem:ivj} we know that, for each term $v$ that is a value in $prg$, $\langle v \rangle$ is a value in $\langle prg \rangle$ and thus the right-hand sides of $\sigma \circ \langle \cdot \rangle$ are values in $\langle prg \rangle$. Consequently, it is $\langle s \rangle = \langle q_r \rangle[\langle \cdot \rangle \circ \sigma] \mapsto \langle t_r \rangle[\langle \cdot \rangle \circ \sigma] = \langle t \rangle$.
\end{proof}
\end{lemma}

\begin{proof}[Proof of \autoref{lem:iso}]
$``\Rightarrow"$: By the derivation of $s \longrightarrow_{prg} t$ there are terms $s', t'$ and an evaluation context $\mathcal{E}$ for $prg$ such that $s = \mathcal{E}[s'], t = \mathcal{E}[t']$ and $s' \mapsto_{prg} t'$. By \autoref{lem:iec} and \autoref{lem:isoc} we know that $\langle \mathcal{E} \rangle$ is an evaluation context for $\langle prg \rangle$ and that $\langle s' \rangle \mapsto_{\langle prg \rangle} \langle t' \rangle$, respectively. Consequently, it is $\langle s \rangle = \langle \mathcal{E} \rangle [\langle s' \rangle] \longrightarrow_{\langle prg \rangle} \langle \mathcal{E} \rangle [\langle t' \rangle] = \langle t \rangle$.

$``\Leftarrow"$: By the $``\Rightarrow"$ direction for $\langle \cdot \rangle^{-1}$.
\end{proof}

To use \autoref{lem:iso} for these transformations, \textsf{CoreDefunc} and $\langle \cdot \rangle^d$ need to be invertible and $\langle \cdot \rangle^d$ may only change symbols but not the structure of terms. The second point is clear since $\langle \cdot \rangle^d$ was defined in such a way. Inverting the transformations is straightforward: For terms, simply use the defining equations backwards. For programs, since $\langle \cdot \rangle^d$ for symbols is required to be invertible (\autoref{fig:termdefunc}), the origin of each left-hand side and each constructor signature can be traced, and thus the definition of \textsf{CoreDefunc} can also be used backwards: We know which functions originally were destructors, and which weren't, and from this we can reconstruct the original codata and some of the function definitions; we also know which constructors originally were functions, and which weren't, and from this we can reconstruct the remaining original function definitions and the original data definitions.

Using \autoref{lem:iso}, we finally prove strong bisimulation for \textsf{CoreDefunc}, by showing that the premiss of the lemma holds.

\begin{lemma}[Strong bisimulation of the rule relations]
\label{lem:sbruld}
\[
\forall s,t \in \textrm{Term}_{prg}. (s, t) \in \textrm{Rules}(prg) \iff (\langle s \rangle^d, \langle t \rangle^d) \in \textrm{Rules}(\langle prg \rangle^{\textsf{CoreDefunc}}).
\]
\begin{proof}
By inspecting the definition of \textsf{CoreDefunc}, it can be easily seen that for each rule $(s, t) \in \textrm{Rules}(prg)$, there is a rule $(\langle s \rangle^d, \langle t \rangle^d) \in \textrm{Rules}(\langle prg \rangle^{\textsf{CoreDefunc}})$, and that there are no other rules in $\textrm{Rules}(\langle prg \rangle^{\textsf{CoreDefunc}})$.
\end{proof}
\end{lemma}

\begin{corollary}[Strong bisimulation of \textsf{CoreDefunc}]
\[
\forall s,t \in \textrm{Term}_{prg}. s \longrightarrow_{prg} t \iff \langle s \rangle^d \longrightarrow_{\langle prg \rangle^{\textsf{CoreDefunc}}} \langle t \rangle^d.
\]
\begin{proof}
By combining \autoref{lem:sbruld} and \autoref{lem:iso}.
\end{proof}
\end{corollary}

\section{Core refunctionalization}
\label{sec:corerefunc}

Core refunctionalization is defined analogously to core defunctionalization, with the exception of needing one more preprocessing, described below. The actual algorithm applies the refunctionalization of \citet{rendel15automatic} for the Data Fragment to the parts of the program that are in this fragment, and leaves the rest, which is already in a refunctionalized form after \textsf{Unnest}, virtually unchanged. Unfortunately, and unlike defunctionalization, this algorithm requires some kind of preprocessing, since the actual refunctionalization requires all constructors to be the first pattern of an lhs. We first describe this asymmetry in more detail and illustrate the underlying problem, then consider a possible workaround for our situation, and finally present an example for and formally define the actual refunctionalization and, as with defunctionalization, consider important properties of it.

\subsection{An asymmetry}

\subsubsection{The problem at hand}
\label{sssec:asym}

\begin{figure}
\begin{subfigure}{0.5\textwidth}
\begin{align*}
fdef_{un,d} &::= eqn_{nd}^* ~ | ~ eqn_d^* \\
eqn_d &::= fun(x^*).des(y^*) = t \\
eqn_{nd} &::= fun(p^*) = t \\
\end{align*}
\caption{For defunctionalization}
\label{fig:unnfddefunc}
\end{subfigure}
\begin{subfigure}{0.5\textwidth}
\begin{align*}
fdef_{un,r} &::= eqn_{nr}^* ~ | ~ eqn_r^* \\
eqn_r &::= fun(x^*).des(y^*)^* = t \\
eqn_{nr} &::= fun(x^*, con(y^*), z^*) = t \\
\end{align*}
\caption{For refunctionalization}
\label{fig:unnfdrefunc}
\end{subfigure}
\caption{Syntax for unnested function definitions}
\end{figure}

In the introduction to this chapter, we gave two fragments of Uroboro induced by unnested function definition fragments, as the domain of defunctionalization and refunctionalization, respectively. The first fragment, for defunctionalization, has the syntax for function definitions shown in \autoref{fig:unnfddefunc}. It is unnested regarding destructors, and allows arbitrary patterns when no destructors are present.

The second fragment, for refunctionalization, with its syntax for function definitions shown in \autoref{fig:unnfdrefunc}, is somewhat analogous to the first, in that it is unnested regarding constructors, and allows arbitrarily many destructors when no constructors are present. However, there is an asymmetry between the two fragments. When only one destructor is allowed, this can also only be placed at one position. On the other hand, allowing only one constructor leaves open the possibility of placing this at different positions. More on this later.

The refunctionalization algorithm defined below requires the constructor to be at the first position, i.e., it has as its domain the fragment $\mathbf{U}_{cc}[fdef_{un,r'}]$ of Uroboro defined as follows.

\begin{definition}[Unnested Fragment for the actual \textsf{CoreRefunc}]
The fragment $\mathbf{U}_{cc}[fdef_{un,r'}]$, for the actual \textsf{CoreRefunc}, is induced by the function definition fragment $fdef_{un,r'}$, defined below using EBNF rules.
\begin{align*}
fdef_{un,r'} &::= eqn_{nr'}^* ~ | ~ eqn_r^* \\
eqn_r &::= fun(x^*).des(y^*)^* = t \\
eqn_{nr'} &::= fun(con(x^*), y^*) = t \\
\end{align*}
\end{definition}

The two fragments $\mathbf{U}_{cc}[fdef_{un,d}]$ and $\mathbf{U}_{cc}[fdef_{un,r'}]$ are the Data and Codata fragment from Rendel et al., extended to allow arbitrary lhss, as long as they are already defunctionalized or refunctionalized, i.e., they contain no destructors or no constructors, respectively. This allows us to extend their de- and refunctionalization transformations for the Codata and Data Fragments, to our core de- and refunctionalization for$\mathbf{U}_{cc}[fdef_{un,r'}]$ and $\mathbf{U}_{cc}[fdef_{un,d}]$, respectively. The Data and Codata Fragment are symmetric to each other, as de- and refunctionalization can switch between them; as \citet{rendel15automatic} describe it, for these fragments, de- and refunctionalization is the same transformation, namely a matrix transposition for a two-dimensional matrix representation of programs.

All this would be good and well, were it not for the fact that unnesting to prepare for refunctionalization doesn't arrive at $\mathbf{U}_{cc}[fdef_{un,r'}]$. A somewhat ad-hoc workaround is presented in the \hyperref[sssec:workaround]{section} following the next. More importantly, however, this situation shines a light on the asymmetry between destructors and constructors---in Uroboro and elsewhere, as this difference is not one exclusive to Uroboro. This underlying problem is illustrated in the next section.

\subsubsection{The general problem}

The general problem is that destructors allow one to specify how to observe the result in only one way at a time, that is, the next destructor in the stack already specifies how to observe the result of the previous, while constructors allow one to inspect input in different ways. As an example, consider the function $fun$ with two arguments $x, y$.
\[
fun(x, y) = ?
\]
In order to refine the definition of $fun$, one might choose to specify how its output is observed. Suppose there are two destructors that fit the type of $fun$, a coverage complete definition thus would be:
\begin{align*}
& fun(x, y).des1 = x \\
& fun(x, y).des2 = y
\end{align*}
Instead of stopping here, this might be refined further, but, when starting refinement with specifying output observation, the refinement will always invariantly start with this step as long as coverage completeness is desired. On the other hand, one might choose to inspect the input of $fun$ by replacing a variable with the constructor calls that fit its type.
\begin{align*}
& fun(con1(), y) = con1() \\
& fun(con2(), y) = y
\end{align*}
Here $x$ was chosen to be inspected. But, unlike with outputs, this isn't the only way to start the refinement, since we could have instead chosen to inspect $y$.
\begin{align*}
& fun(x, con()) = con() \\
\end{align*}

The problem is directly related to the difference between natural deduction and arbitrary sequent calculus. In natural deduction, the left-hand side sequence consists of many elements, while the right-hand side sequence consists of only one element. Similarly, the many sets of constructors for each type of the variables allow many ``dimensions'' of the input to be observed, while the set of destructors for the codata type of a function only allows one ``dimension'' of the output to be observed.

\subsubsection{A workaround}
\label{sssec:workaround}

Since solving the underlying problem presented above is beyond the scope of our work, we will just present a workaround for the problem at hand, which is, transforming a program in the fragment $\mathbf{U}_{cc}[fdef_{un,r}]$ to a semantically equivalent program in fragment $\mathbf{U}_{cc}[fdef_{un,r'}]$. We start with a naive approach, which doesn't preserve the semantics, and then refine this approach using an extraction.

Consider the following program. It is in fragment $\mathbf{U}_{cc}[fdef_{un,r}]$, but not in $\mathbf{U}_{cc}[fdef_{un,r'}]$, since the constructors in the function definition for $f$ are not at the first argument position.

\begin{lstlisting}
data P where
  c1(P): P
  c2(): P

codata N where
  N.des(): N

function g(): N where
  g() = g()

function h(): P where
  h() = c2()

function f(N, P): P where
  f(x, c1(y)) = c1(y)
  f(x, c2()) = c2()

function main(): P where
  main() = f(g(), c1(h()))
\end{lstlisting}

What is the easiest way to bring this to the form required by $\mathbf{U}_{cc}[fdef_{un,r'}]$? Arguably, the approach that requires the minimal amount of syntactic change is to simply switch the arguments of $f$, in the signature and equations of the function definition and at the call sites. The resulting program looks as follows (we omit the unchanged data and codata type definitions).

\begin{lstlisting}
function g(): N where
  g() = g()

function h(): P where
  h() = c2()

function f(P, N): P where
  f(c1(y), x) = c1(y)
  f(c2(), x) = c2()

function main(): P where
  main() = f(c1(h()), g())
\end{lstlisting}

However, this also changes the semantics of the program. For instance, consider the term \texttt{main()}. In the original program, this term reduces as follows:
\[
\texttt{main()} \longrightarrow \texttt{f(g(), c1(h()))} \longrightarrow \texttt{f(g(), c1(h()))} \longrightarrow ...
\]
The equation \texttt{g() = g()} leads to an infinite reduction sequence. Thus, under left-to-right call-by-value semantics, the equation \texttt{h() = c2()} can never be used. Now, consider how \texttt{main()} reduces in the transformed program:
\[
\texttt{main()} \longrightarrow \texttt{f(c1(h()), g())} \longrightarrow \texttt{f(c2(), g())} \longrightarrow \texttt{f(c2(), g())} \longrightarrow ...
\]
Eventually, we again have an infinite reduction sequence, but before that, the equation \texttt{h() = c2()} is used, unlike in the original program. The reason is that the term \texttt{g()}, that triggers the infinite sequence, cannot prevent the reduction by this equation anymore since it is now in the second argument position, behind \texttt{c1(h())}.

How to fix this? By first extracting the constructor, and then switching the arguments of the auxiliary function. Under this approach, the transformed program looks as follows (again omitting the unchanged (co)data type definitions):

\begin{lstlisting}
function g(): N where
  g() = g()

function h(): P where
  h() = c2()

function f(N, P): P where
  f(x, y) = aux(y, x)

function aux(P, N): P where
  aux(c1(y), x) = c1(y)
  aux(c2(), x) = c2()

function main(): P where
  main() = f(g(), c1(h()))
\end{lstlisting}

How about reducing \texttt{main()} in this program? The reduction sequence we get is no different from that of the original program.
\[
\texttt{main()} \longrightarrow \texttt{f(g(), c1(h()))} \longrightarrow \texttt{f(g(), c1(h()))} \longrightarrow ...
\]
And this is no surprise, since we didn't change the order of the arguments of $f$, but rather those of $aux$. In general, this approach preserves the semantics of a program in the weak bisimilarity presented for extractions, for the following reasons.
\begin{enumerate}
\item Extracting the constructor preserves the semantics in this weak bisimilarity, as proven in \autoref{ch:extr}, as long as both the original and the transformed program have no overlapping equations. For the original program, this holds since it has copattern coverage by assumption. The transformed program again has copattern coverage, for the same reasons as presented for the variable splitting case of the unnesting algorithm.

\item Switching the arguments of the auxiliary function doesn't change the semantics, when one only considers reductions starting with terms of the original program.
\end{enumerate}
As this solution is rather ad-hoc, we don't go into the details of how to prove the bisimulation.

\subsection{The actual algorithm}

Now, we turn to the actual refunctionalization algorithm, with domain $\mathbf{U}_{cc}[fdef_{un,r'}]$.

\subsubsection{Example}

\begin{figure}
\begin{lstlisting}
data Nat where
  Zero(): Nat
  Succ(Nat): Nat

function add(Nat, Nat): Nat where
  add(Zero(), n) = n
  add(Succ(m), n) = Succ(add(m, n))

function multiply(Nat, Nat): Nat where
  multiply(Zero(), n) = Zero()
  multiply(Succ(m), n) = add(multiply(m, n), n)

codata Array where
  Array.get(Nat): Nat

function oneElemArray(Nat): Array where
  oneElemArray(n).get(m) = aux(n, m)

function aux(Nat, Nat): Nat where
  aux(n, m) = m.aux3(n)

function aux3(Nat, Nat): Nat where
  aux3(Zero(), n) = n
  aux3(Succ(m), n) = Zero()

function isOne(Nat): Nat where
  isOne(Zero()) = Zero()
  isOne(Succ(n)) = aux2(n)

function aux2(Nat): Nat where
  aux2(Zero()) = Succ(Zero())
  aux2(Succ(n)) = Zero()
\end{lstlisting}
\caption{Uroboro sample before core refunctionalization}
\label{fig:ch4urosampleprerefunc}
\end{figure}

\begin{figure}
\begin{lstlisting}
codata Nat where
  Nat.add(Nat): Nat
  Nat.multiply(Nat): Nat
  Nat.aux3(Nat): Nat
  Nat.isOne(): Nat
  Nat.aux2(): Nat

function Zero(): Nat where
  Zero().add(n) = n
  Zero().multiply(n) = Zero()
  Zero().aux3(Nat) = n
  Zero().isOne() = Zero()
  Zero().aux2() = Succ(Zero())

function Succ(Nat): Nat where
  Succ(m).add(n) = Succ(m.add(n))
  Succ(m).multiply(n) = m.multiply(n).add(n)
  Succ(m).aux3(n) = Zero()
  Succ(n).isOne() = n.aux2()
  Succ(n).aux2() = Zero()

codata Array where
  Array.get(Nat): Nat

function oneElemArray(Nat): Array where
  oneElemArray(n).get(m) = aux(n, m)

function aux(Nat, Nat): Nat where
  aux(n, m) = m.aux3(n)
\end{lstlisting}
\caption{Uroboro sample after core refunctionalization}
\label{fig:ch4urosamplepostrefunc}
\end{figure}

Before formally defining \textsf{CoreRefunc}, like with defunctionalization, we continue the example from unnesting, here with the result of applying $\textsf{Unnest}_r$ to it. The auxiliary function $aux$ generated by it has to be subjected to the workaround presented above, since its lhss have constructors at non-initial argument positions. After applying this workaround, the program looks as shown in \autoref{fig:ch4urosampleprerefunc}.

The only function definitions which are already refunctionalized, in the sense that their lhss contain no constructors, are \texttt{oneElemArray} and \texttt{aux}. All other function definitions dispatch on their first argument of type \texttt{Nat}; \textsf{CoreRefunc} turns data type \texttt{Nat} into a codata type with a destructor replacing each of these functions, and produces function definitions replacing the constructors \texttt{Zero()} and \texttt{Succ(Nat)}. These function definitions collect all equations which have the respective constructor on their lhs; the result is shown in \autoref{fig:ch4urosamplepostrefunc}. As desired, all data types and constructor patterns have been eliminated.

\subsubsection{Formal definition}

\begin{figure}
\begin{align*}
\langle x \rangle^r = x \\
\langle s.des(t_1, ..., t_n) \rangle^r = \langle s \rangle^r .des(\langle t_1 \rangle^r, ..., \langle t_n \rangle^r) \\
\langle fun(t_1, ..., t_n) \rangle^r = fun(\langle t_1 \rangle^r, ..., \langle t_n \rangle^r), \text{ if } fun \in F^{prg}_r \\
\langle fun(t_1, ..., t_n) \rangle^r = \langle t_1 \rangle^r .\langle fun \rangle^r (\langle t_2 \rangle^r, ..., \langle t_n \rangle^r), \text{ if } fun \not\in F^{prg}_r \\
\langle con(t_1, ..., t_n) \rangle^r = \langle con \rangle^r (\langle t_1 \rangle^r, ..., \langle t_n \rangle^r)
\end{align*}
For a symbol, $\langle \cdot \rangle^r$ means that the syntactic domain is changed: $\langle con \rangle^r$ is a function symbol, and for $fun \not\in F^{prg}_r$, $\langle fun \rangle^r$ is a destructor symbol. The transformation for symbols must be invertible, but the name of the new symbol is otherwise arbitrary.
\caption{Refunctionalization of terms}
\label{fig:refuncterm}
\end{figure}

First, we define the class of function definitions of a program $prg$ which are already refunctionalized. We distinguish between empty function definition, that is, those without any equations, and non-empty function definitions. A non-empty function definition is already refunctionalized when (a) all of its equations have destructor patterns, or (b) the function has no arguments, or (c) the first argument of every lhs is a variable. An empty function definition is already refunctionalized when the function's return type is a negative (codata) type. Copattern coverage for such empty function definitions is derived by result splitting. Note that, just with defunctionalization, there is some degree of freedom in the definition of already refunctionalized empty function definitions: One might choose to only regard those empty function definitions as already refunctionalized when none of its argument types is an empty data type. We define the class of already refunctionalized functions $F^{prg}_r$ as those which have already defunctionalized function definitions.

Using this definition, in \autoref{fig:refuncterm} we define refunctionalization of terms, dually to defunctionalization of terms.

Next, we define \textsf{CoreRefunc}.

\begin{algorithm}[\textsf{CoreRefunc}]

\begin{align*}
\langle prg \rangle^{\textsf{CoreDefunc}} & = ~&& \{ && \textrm{\textbf{codata }} \sigma \textrm{\textbf{ where }} \span\span\span\span \\
& && && && \{ \sigma.\langle fun \rangle^d (\tau_1, ..., \tau_n): \tau ~ \\
& && && && | ~ `` fun(\sigma, \tau_1, ..., \tau_n): \tau " \in prg \text{ s.t. } fun \not\in F^{prg}_d \} \\
& && | && `` \textrm{\textbf{data }} \sigma ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} \langle con \rangle^r (\tau_1, ..., \tau_n): \tau \textrm{\textbf{ where}} \span\span\span\span \\
& && && && \{ \langle con \rangle^r (\langle fun \rangle^d (\overline{x}), \overline{y}) = \langle t \rangle^d ~ | ~ `` fun(con(\overline{x}), \overline{y}) = t " \in prg \} \\
& && | && `` con(\tau_1, ..., \tau_n): \tau " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{codata }} ... ~ | ~ `` \textrm{\textbf{codata }} ... " \in prg \} \span\span\span\span \\
& \cup && \{ && \textrm{\textbf{function }} fun(\tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} \{ q = \langle t \rangle^r ~ | ~ ``q = t" \in eqns \} \span\span\span\span \\
& && | && `` \textrm{\textbf{function }} fun(\tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} eqns " \text{ s.t. } fun \in F^{prg}_r \} \span\span\span\span
\end{align*}

\end{algorithm}

\subsection{Properties}

We consider important properties of the transformation.

\textbf{Termination.} The argument here is virtually identical to that for \textsf{CoreDefunc}; just replace ``codata signatures'' with ``data signatures'' and ``destructor signatures'' with ``constructor signatures''.

\textbf{Range.} This is also shown completely analogously to \textsf{CoreDefunc}, again using the respective dual notions. It can be directly seen in the definition that the resulting program contains no data type definitions. The copattern coverage of the resulting program is also easy to see. In the set-builder definition, the last set in the union contains function definitions taken over from the original program with only the right-hand sides of their equations changed (by $\langle \cdot \rangle^r$); since the left-hand sides stay the same, these function definitions again have copattern coverage. The function definitions from the second set in the union remain to be considered. For each of their equations, each of its left-hand sides is constructed (disregarding the receiver notation for destructors) from the original left-hand side by turning the function name into the corresponding destructor name and the constructor name into the corresponding function name. For the original function definitions, copattern coverage was derived by variable splitting on the first variable; consequently, copattern coverage for the new function definitions can be derived by result splitting.

\textbf{Preservation of well-typedness.} Again, this is dual to \textsf{CoreDefunc}. Let $prg$ be the program to be transformed; we assume $prg$ to be well-typed. This means that for each equation $eqn = `` q = t ''$ of $prg$ we have a derivation of $\Sigma_{prg} \vdash eqn \textrm{ ok}$, with $\Sigma_{prg}$ being the signatures of $prg$. We will show that each equation of $\langle prg \rangle^{\textsf{CoreRefunc}}$ is well-typed. In the definition of \textsf{CoreRefunc}, we can see that the signatures $\Sigma_{\langle prg \rangle^{\textsf{CoreRefunc}}}$ of $\langle prg \rangle^{\textsf{CoreRefunc}}$ are exactly the following: for each constructor signature in $\Sigma_{prg}$ an identical function signature, for some function signatures in $\Sigma_{prg}$ identical destructor signatures, and for the other function signatures in $\Sigma_{prg}$ the identical function signatures. From this, we have the following lemma.

\begin{lemma}
\label{lem:pwtrefunc}
When $\Sigma_{prg} \vdash `` q = t " \textrm{ ok}$, then $\Sigma_{\langle prg \rangle^{\textsf{CoreRefunc}}} \vdash `` \langle q \rangle^r = \langle t \rangle^r " \textrm{ ok}$.
\begin{proof}
By induction on the derivation of $\Sigma \vdash `` q = t " \textrm{ ok}$, using the similarity of $\Sigma_{prg}$ and $\Sigma_{\langle prg \rangle^{\textsf{CoreRefunc}}}$.
\end{proof}
\end{lemma}

In the definition of \textsf{CoreRefunc} we can see that each equation $eqn = `` q = t "$ in $prg$ corresponds to exactly one equation $`` \langle q \rangle^r = \langle t \rangle^r "$ of $\langle prg \rangle^{\textsf{CoreRefunc}}$. Combine this with \autoref{lem:pwtrefunc} to get $\Sigma_{\langle prg \rangle^{\textsf{CoreRefunc}}} \vdash `` q = t " \textrm{ ok}$ for each equation $eqn$ of $\langle prg \rangle^{\textsf{CoreRefunc}}$. Thus $\langle prg \rangle^{\textsf{CoreRefunc}}$ is well-typed whenever $prg$ is.

\textbf{Bisimulation.} For \textsf{CoreRefunc}, strong bisimulation holds. Again, this is shown completely analogously to \textsf{CoreDefunc}; we reuse \autoref{lem:iso}. To use this lemma, as with \textsf{CoreDefunc}, \textsf{CoreRefunc} and $\langle \cdot \rangle^r$ must be invertible, and $\langle \cdot \rangle^r$ may not change the structure of the terms, only the symbols. The second point follows directly from the definition of $\langle \cdot \rangle^r$. Inverting the transformations is completely analogous to \textsf{CoreDefunc}: Since $\langle \cdot \rangle^r$ for symbols is invertible, the origin of each term, each lhs and each signature can be traced back and thus the definitions of $\langle \cdot \rangle^r$ and \textsf{CoreRefunc} can be used backwards.

\begin{lemma}[Strong bisimulation of the rule relations]
\label{lem:sbrulr}
\[
\forall s,t \in \textrm{Term}_{prg}. (s, t) \in \textrm{Rules}(prg) \iff (\langle s \rangle^r, \langle t \rangle^r) \in \textrm{Rules}(\langle prg \rangle^{\textsf{CoreRefunc}}).
\]
\begin{proof}
By inspecting the definition of \textsf{CoreRefunc}, it can be easily seen that for each rule $(s, t) \in \textrm{Rules}(prg)$, there is a rule $(\langle s \rangle^r, \langle t \rangle^r) \in \textrm{Rules}(\langle prg \rangle^{\textsf{CoreRefunc}})$, and that there are no other rules in $\textrm{Rules}(\langle prg \rangle^{\textsf{CoreRefunc}})$.
\end{proof}
\end{lemma}

\begin{corollary}[Strong bisimulation of \textsf{CoreRefunc}]
\[
\forall s,t \in \textrm{Term}_{prg}. s \longrightarrow_{prg} t \iff \langle s \rangle^r \longrightarrow_{\langle prg \rangle^{\textsf{CoreRefunc}}} \langle t \rangle^r.
\]
\begin{proof}
By combining \autoref{lem:sbrulr} and \autoref{lem:iso}.
\end{proof}
\end{corollary}
