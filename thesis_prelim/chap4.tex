\chapter{Automatic de- and refunctionalization}

%%-- under construction

...

We present an algorithm that defunctionalizes arbitrary Uroboro programs with copattern coverage, and an algorithm that refunctionalizes such programs. Both algorithms are made up of two phases, in order:
\begin{enumerate}
\item Unnesting, and
\item core de-/refunctionalization.
\end{enumerate}

In short, the entire process can be described as follows. By unnesting them as far as necessary, the preprocessing phases brings the lhss of the program to the form accepted by the core de-/refunctionalization, which is essentially the two-way transformation of Rendel et al.

The unnesting is done by a parameterized algorithm $\textsf{Unnest}_i$, where the parameter $i$ can be either $d$ or $r$, standing for defunctionalization and refunctionalization, respectively. The parameter determines when the algorithm may stop, such that its results are legal inputs for de- or refunctionalization. The unnesting algorithm is essentially the copattern unnesting algorithm of Setzer et al., adapted to Uroboro.

We give the intended domain and range for each of the algorithms, all of which are specific fragments of Uroboro.
\begin{align*}
& \textsf{Unnest}_i: \mathbf{U}_{cc} \to \mathbf{U}_{cc}[fdef_{un,i}] \text{ for } i \in \{d,r\} \\
& \textsf{CoreDefunc}: \mathbf{U}_{cc}[fdef_{un,d}] \to \mathbf{U}^{defunc}_{cc} \\
& \textsf{CoreRefunc}: \mathbf{U}_{cc}[fdef_{un,r}] \to \mathbf{U}^{refunc}_{cc}
\end{align*}

\begin{definition}[Unnested Fragments for \textsf{CoreDefunc} and \textsf{CoreRefunc}]
The fragments $\mathbf{U}_{cc}[fdef_{un,d}]$, $\mathbf{U}_{cc}[fdef_{un,r}]$, for lhss unnested as far as necessary for \textsf{CoreDefunc} and \textsf{CoreRefunc}, respectively, are induced by the function definition fragments $fdef_{un,d}$ and $fdef_{un,r}$, respectively, both defined below using EBNF rules. For these, the basic EBNF rules from the definition of the syntax of Uroboro are reused.
\begin{align*}
fdef_{un,d} &::= eqn_{nd}^* ~ | ~ eqn_d^* \\
eqn_{nd} &::= fun(x^*).des(y^*) = t \\
eqn_d &::= fun(p^*) = t \\
\end{align*}
\begin{align*}
fdef_{un,r} &::= eqn_{nr}^* ~ | ~ eqn_r^* \\
eqn_r &::= fun(x^*).des(y^*)^* = t \\
eqn_{nr} &::= fun(con(x^*), y^*) = t \\
\end{align*}
\end{definition}

For each algorithm it will be shown, in its respective section, that it actually has the range specified above. In total, three important properties will be shown for each algorithm: Termination, correct range, and preservation of semantics in a (weak) bisimulation.

The rest of the section is organized as follows. The first section describes the unnesting algorithm, shows why the three properties hold, and gives applies it to an example. Section 2 and 3 do the same for the unmixing algorithm, and core de-/refunctionalization, respectively.

\section{Unnesting}

We adapt the translation algorithm from section 3.3 of the paper of Setzer et al. to Uroboro. Their algorithm translates equations step-by-step depending upon the derivation of the copattern coverage of their lhss. Our adaptation only has steps which concern patterns that have a counterpart in Uroboro\footnote{I.e., not those for applications, units, and pairs. In the adapted algorithm for Uroboro, the last one is merged into the steps for destructors and constructors, while the others are irrelevant because Uroboro doesn't have the corresponding higher-order constructs.}. Each such translation step can be considered an extraction as defined in chapter 3.

\subsection{Simple patterns}

Before we start with the translation algorithm, we adapt and introduce, respectively, two notions of simplicity for patterns, the second of which corresponds to the intended range of the translation algorithm. We first adapt the definition of a \textit{simple pattern}, as given by Setzer et al., to Uroboro. Call a copattern simple if it is of one of the three forms $fun(\overline{x})$, $fun(\overline{x}).des(\overline{y})$, or $fun(con(\overline{x}), \overline{y})$.

Next, we generalize this definition to define \textit{sufficiently simple patterns} for a purpose, which is either de- or refunctionalization. Call a copattern \textit{sufficiently simple for defunctionalization} if it one of the two forms $fun(\overline{x}).des(\overline{y})$, $fun(\overline{p})$, and \textit{sufficiently simple for refunctionalization} if it one of the two forms $fun(\overline{x}).\overline{des(\overline{y})}$, $fun(con(\overline{x}), \overline{y})$. We will also say that a copattern is sufficiently simple for $i$, for $i \in \{d,r\}$, and mean that it is sufficiently simple for defunctionalization or refunctionalization, respectively.

Those notions correspond directly to the forms of lhss allowed in the unnested fragments for \textsf{CoreDefunc} and \textsf{CoreRefunc}, respectively. It is clear that each simple copattern is also sufficiently simple for both de- and refunctionalization.

\subsection{Translation algorithm}

We describe the algorithm for $\textsf{Unnest}_i$, adapted from section 3.3 of the paper of Setzer et al. to Uroboro, as lined out above.

\begin{algorithm}[$\textsf{Unnest}_i$]

Like Setzer et al., we consider the last step in the derivation of the copattern coverage for the lhss of a function definition. Just like them, we are not interested in the case where the last step is C\textsubscript{Head}, since that means the lhs is already simple, and we terminate. When the last step isn't C\textsubscript{Head}, but the lhss are all sufficiently simple for $i$ nonetheless, we also terminate.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q)$}
\RightLabel{\textbf{C}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q_i)_{i \in I}$}
\end{prooftree}

Also like Setzer et al., we then introduce a fresh auxiliary function and translate the program depending on \textbf{C}. As stated before, these translations are slightly adapted to fit the framework of Uroboro, such that two remain, one for destructors and one for constructors. More importantly, both of those are in fact extractions as defined in chapter 3, namely destructor and constructor extraction. 

\begin{enumerate}
\item \textbf{C} is C\textsubscript{Des}, and the last derivation step looks as follows.

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q^\tau)$}
\RightLabel{C\textsubscript{Des}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q.des(\overline{x^{des}}))_{des \in Dess_\tau}$}
\end{prooftree}

Apply extraction \textsf{ExtractDes} targeting the equations with lhss $q.des(\overline{x^{des}})$, for all $des \in Dess_\tau$, to the function definition for $fun$. From this we obtain the changed function definition $def'_{fun}$ for $fun$ and the auxiliary function definition. The lhss of $def'_{fun}$ are
\[
Q \cup \{ \texttt{get}(q.des(\overline{x^{des}})) ~ | ~ des \in Dess_\tau \} = Q \cup \{q\},
\]
where \texttt{get} is the retrieving part of the lens underlying \textsf{ExtractDes}. This set of lhss is the same as that in $fun \lhd | ~ Q ~ (q^\tau)$ from the derivation step above. The lhss of the auxiliary function definition are
\begin{multline*}
\{ q_{\zeta_r} ~ | ~ r \in T \} = \{ \texttt{putback}(\langle \texttt{get}(q_r) \rangle^{aux}, q_r) ~ | ~ r \in T \} \\
= \{ \sigma^{q_r}_\pi(aux(\langle q \rangle^{vars})) ~ | ~ r \in T \} = \{ aux(\langle q \rangle^{vars}).des(\overline{x^{des}}) ~ | ~ des \in Dess_\tau \} \\
= \{ aux(\langle q \rangle^{vars}).des(\overline{x^{des}}) ~ | ~ des \in Dess_\tau \},
\end{multline*}
where $\zeta$ is the function in the triple that is \textsf{ExtractCon}(p). It is clear that each $q_{\zeta_r}$ is simple and that $\{ q_{\zeta_r} ~ | ~ r \in T \}$ covers $aux$:
\begin{prooftree}
\AxiomC{}
\RightLabel{C\textsubscript{Head}}
\UnaryInfC{$aux \lhd | ~ aux(x, \overline{y})$}
\RightLabel{C\textsubscript{ResSplit}}
\UnaryInfC{$aux \lhd | ~ (q_{\zeta_r})_{r \in T}$}
\end{prooftree}

\item \textbf{C} is C\textsubscript{Con}, and the last derivation step looks as follows. 

\begin{prooftree}
\AxiomC{$fun \lhd | ~ Q ~ (q(x^\tau))$}
\RightLabel{C\textsubscript{Con}}
\UnaryInfC{$fun \lhd | ~ Q ~ (q[x := con(\overline{y^{con}})])_{con \in Cons_\tau}$}
\end{prooftree}

Let $p$ be the position of $x$ in $q$. Apply extraction \textsf{ExtractCon}(p) targeting the equations with lhss $q[x := con(\overline{y^{con}})]$, for all $con \in Cons_\tau$ to the original function definition for $fun$. From this we obtain the changed function definition $def'_{fun}$ for $fun$ and the auxiliary function definition. The lhss of $def'_{fun}$ are
\[
Q \cup \{ \texttt{get}(q[x := con(\overline{y^{con}})]) ~ | ~ con \in Cons_\tau \} = Q \cup \{q\},
\]
where the pair of \texttt{get} and \texttt{putback} is the lens underlying \textsf{ExtractCon}(p). This set of lhss is the same as that in $fun \lhd | ~ Q ~ (q)$ from the derivation step above. The lhss of the auxiliary function definition are
\begin{multline*}
\{ q_{\zeta_r} ~ | ~ r \in T \} = \{ \texttt{putback}(\langle \texttt{get}(q_r) \rangle^{aux}, q_r) ~ | ~ r \in T \} \\
= \{ \sigma^{q_r}_\pi(aux(\langle q \rangle^{vars})) ~ | ~ r \in T \} = \{ aux(\langle q \rangle^{vars})[x := con(\overline{y^{con}})] ~ | ~ con \in Cons_\tau \} \\
= \{ aux(x, \langle q_r \rangle^{vars})[x := con(\overline{y^{con}})] \text{ for some } r \in T ~ | ~ con \in Cons_\tau \},
\end{multline*}
where $\zeta$ is the function in the triple that is \textsf{ExtractCon}(p). It is clear that each $q_{\zeta_r}$ is simple and that $\{ q_{\zeta_r} ~ | ~ r \in T \}$ covers $aux$:
\begin{prooftree}
\AxiomC{}
\RightLabel{C\textsubscript{Head}}
\UnaryInfC{$aux \lhd | ~ aux(x, \overline{y})$}
\RightLabel{C\textsubscript{VarSplit}}
\UnaryInfC{$aux \lhd | ~ (q_{\zeta_r})_{r \in T}$}
\end{prooftree}
\end{enumerate}

As shown for each of the two cases, after the extractions copattern coverage still holds for the translated program, and we even know its derivation tree. Thus we can go back to the start and do further translations, if necessary.
\end{algorithm}

Next, we consider important properties of the translation algorithm.

\textbf{Range.} The algorithm only stops when all lhss are sufficiently simple for $i$ and works on the coverage derivation trees, thus we have the desired range $\mathbf{U}_{cc}[fdef_{un,i}]$ if it terminates in all cases, which is shown next.

\textbf{Termination.} To show that the algorithm terminates, we can basically repeat the argument of Setzer et al. Each translation results in a changed function definition $def'_{fun}$ for $fun$ and a new function definition $def_{aux}$ for the auxiliary function. The lhss of $def'_{fun}$ are $Q \cup \{q\}$, which means that they cover $fun$ in the same way $fun$ is covered via $fun \lhd | ~ Q ~ (q)$ in the derivation step above. This means that the translation reduces the depth of the derivation tree for the coverage of $fun$, thus eventually arriving at sufficiently simple patterns. The added auxiliary function definitions already have simple lhss, thus they are not translated, and eventually the algorithm terminates.

\textbf{Bisimulation.} Because copattern coverage holds for the translated program, the translated program has no overlapping lhss. Therefore, by the propositions 3.3.1 and 3.3.2, we know that the translation preserves the semantics of the program in the kind of weak bisimulation described in section 3.3.

\subsection{Example}

As an example application of the algorithm, consider the following program fragment.

\begin{lstlisting}

fun(x).des().des1(con1()) = t1
fun(x).des().des1(con2()) = t2
fun(x).des().des2() = t3

\end{lstlisting}

The final step in the coverage derivation for $fun$ is splitting \texttt{fun(x).des().des1(y)} into \texttt{fun(x).des().des1(con1())} and \texttt{fun(x).des().des1(con2())} by splitting variable $y$. Therefore we first target these two lhss for constructor extraction at the common position of \texttt{con1()} and \texttt{con2()} in the two lhss; the result of the extraction can be seen below.

\begin{lstlisting}

fun(x).des().des1(y) = aux(y, x)
fun(x).des().des2() = t3

aux(con1(), x) = t1
aux(con2(), x) = t2

\end{lstlisting}

For this program, the final step in the coverage derivation for $fun$ is splitting \texttt{fun(x).des()} into \texttt{fun(x).des().des1(y)} and \texttt{fun(x).des().des2()} by result splitting. Therefore we target these two lhss for destructor extraction; the result of the extraction can be seen below. Note that \texttt{fun(x).des() = aux2(x)} is generated twice but only present once in the function definition, as these are sets.

\begin{lstlisting}

fun(x).des() = aux2(x)

aux2(x).des1(y) = aux(y, x)
aux2(x).des2() = t3

\end{lstlisting}

The \texttt{aux} function definition remains unchanged and is omitted. This program is in the desired fragment; the algorithm terminates here because all lhss of the program are simple.

\section{Core de-/refunctionalization}

...

\textsf{CoreDefunc} and \textsf{CoreRefunc} use -- and in fact essentially are -- the respective sides of the two-way transformation of Rendel et al. One side of this is the defunctionalization for the Codata Fragment, which we call $d^{codata}$ in here, and the other side is the refunctionalization for the Data Fragment, which we call $r^{data}$. Their definitions are repeated below for the convenience of the reader.

...

\subsection{Core defunctionalization}

The core defunctionalization \textsf{CoreDefunc} applies the defunctionalization of Rendel et al. for the Codata Fragment to the parts of the program that are in this fragment, and leaves the rest, which is already in a defunctionalized form after \textsf{Unnest}, virtually unchanged. That is, the only change to these parts affects the right-hand sides of equations, which are transformed to account for the syntactic differences between function and destructor calls.\footnote{I.e., the difference between $fun(...)$ and $t.des(...)$. This difference vanishes when changing the syntax of destructor calls to $des(t, ...)$.} The respective transformation for terms is $d$, defined below with the algorithm.

\begin{algorithm}[\textsf{CoreDefunc}]

First, we define the class $FDEF^{prg}_d$ of function definitions of a program $prg$ which are already defunctionalized. A function definition is already defunctionalized when all of its equations have lhss without destructors. We define the class of already defunctionalized functions $F^{prg}_d$ as those which have already defunctionalized function definitions.

Next, we give the definition for \textsf{CoreDefunc}. This needs further explanation due to technical difficulties, as follows below.

\begin{alignat*}{3}
\langle prg \rangle^{\textsf{CoreDefunc}} & = ~&& \langle && \{ def \in prg ~ | ~ def = ``\textbf{codata } ..." \text{ or } def = ``\textbf{function } ..." \not\in F^{prg}_d \} \rangle^{d^{codata}} \\
& \cup && \{ && \textrm{\textbf{data }} ... ~ | ~ `` \textrm{\textbf{data }} ... " \in prg \} \\
& \cup && \{ && \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} \{ q = \langle t \rangle^d ~ | ~ "q = t" \in eqns \} \\
& && | && `` \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} eqns " \in F^{prg}_d \} 
\end{alignat*}

This definition is not quite legal, because the argument of $d^{codata}$ possibly has equations with right-hand sides which cannot be correctly transformed by $d^{codata}$. There are two different problems which can possibly arise, but which are also easily circumvented, as follows.
\begin{enumerate}
\item There are function calls to functions which are already defunctionalized (those in $F_d$); $d^{codata}$ wants to transform them into constructor calls, which is neither possible nor necessary. To circumvent this, replace each function name $fun \in F_d$ (in a right-hand side term) with a destructor name $des_{fun}$ unique to $fun$, and set $\langle des_{fun} \rangle^d = fun$. Then $d^{codata}$ will transform them back into the original function calls, which means that they ultimately are left unchanged, as desired.

\item There are constructor calls; the domain of $d^{codata}$, the Codata Fragment, doesn't allow them. To circumvent this, replace such calls to a constructor $con$ with function calls with a function name $fun_{con}$ unique to $con$, and set $\langle fun_{con} \rangle^d = con$. Then $d^{codata}$ will transform them back into the original constructor calls, which means that they ultimately are left unchanged, as desired.
\end{enumerate}

Below, we define the transformation for terms $d$, which is only necessary due to the syntactic differences between function and destructor calls.

\begin{align*}
\langle x \rangle^d = x \\
\langle s.des(t_1, ..., t_n) \rangle^d = \langle des \rangle^d (\langle s \rangle^d, \langle t_1 \rangle^d, ..., \langle t_n \rangle^d) \\
\langle fun(t_1, ..., t_n) \rangle^d = fun(\langle t_1 \rangle^d, ..., \langle t_n \rangle^d), \text{ if } fun \in F_d \\
\langle fun(t_1, ..., t_n) \rangle^d = \langle fun \rangle^d (\langle t_1 \rangle^d, ..., \langle t_n \rangle^d), \text{ if } fun \not\in F_d \\
\langle con(t_1, ..., t_n) \rangle^d = con(\langle t_1 \rangle^d, ..., \langle t_n \rangle^d)
\end{align*}

\end{algorithm}

We consider important properties of the transformation.

\textbf{Bisimulation.} For \textsf{CoreDefunc}, strong bisimulation holds. The proof relies on properties of $d^{codata}$. Note that the domain of $d^{codata}$ is Rendel et al.'s Codata Fragment. As stated in section 2.3.1, the reduction relation of the Codata Fragment in the work of Rendel et al. and our work's standard reduction relation, restricted to this fragment, are the same, as long as programs are required to have copattern coverage. Analogously, the domain of $r^{data}$ is the Data Fragment, and Rendel et al.'s reduction relation for this fragment is the same as ours restricted to this fragment. In section 3 of their paper, Rendel et al. prove Lemma 5, which in terms of our work can be stated as follows.

\begin{lemma}[Strong bisimulation for $d^{codata}, r^{data}$]
\[
s \longrightarrow_{prg} t \iff \langle s \rangle^d \longrightarrow_{\langle prg \rangle^{d^{codata}}} \langle t \rangle^d
\]
and
\[
s \longrightarrow_{prg} t \iff \langle s \rangle^r \longrightarrow_{\langle prg \rangle^{r^{codata}}} \langle t \rangle^r.
\]
\end{lemma}

Using this, we prove strong bisimulation for \textsf{CoreDefunc}.

\begin{lemma}[Strong bisimulation for \textsf{CoreDefunc}]
\[
s \longrightarrow_{prg} t \iff \langle s \rangle^d \longrightarrow_{\langle prg \rangle^{\textsf{CoreDefunc}}} \langle t \rangle^d
\]

\begin{proof}
For the sake of this proof, we will neglect the syntactic differences between destructor and function calls. By the definition of \textsf{CoreDefunc}, it is
\[
\langle prg \rangle^{\textsf{CoreDefunc}} = \langle prg' \rangle^{d^{codata}} \cup prg''
\]
such that $prg' \cup prg'' = prg$. By Lemma 4.2.1, the reduction relations pertaining to the programs $prg'$ and $\langle prg' \rangle^{d^{codata}}$ are one and the same. The reduction relation pertaining to $prg$ is that of $prg'$ changed according to the equations of $prg''$, and the reduction relation pertaining to $\langle prg \rangle^{\textsf{CoreDefunc}}$ is also that of $prg'$ changed according to the equations of $prg''$. Thus these reduction relations are the same, which is equivalent to the statement of this lemma.
\end{proof}
\end{lemma}

\subsection{Core refunctionalization}

Core refunctionalization is defined analogously to core defunctionalization. It applies the refunctionalization of Rendel et al. for the Data Fragment to the parts of the program that are in this fragment, and leaves the rest, which is already in a refunctionalized form after \textsf{Unnest}, virtually unchanged. 

\begin{algorithm}[\textsf{CoreRefunc}]

First, we define the class $FDEF^{prg}_r$ of function definitions of a program $prg$ which are already refunctionalized. A function definition is already refunctionalized when (a) all of its equations have destructor patterns, or (b) the function has no arguments, or (c) the first argument of every lhs is a variable. We define the class of already refunctionalized functions $F^{prg}_r$ as those which have already defunctionalized function definitions.

Next, we give the definition for \textsf{CoreRefunc}. This needs further explanation due to technical difficulties, as follows below.

\begin{alignat*}{3}
\langle prg \rangle^{\textsf{CoreRefunc}} & = ~&& \langle && \{ def \in prg ~ | ~ def \textrm{ is data def. or function def. } \not\in FDEF^{prg}_r \rangle^{r^{data}} \\
& \cup && \{ && \textrm{\textbf{codata }} ... ~ | ~ `` \textrm{\textbf{codata }} ... " \in prg \} \\
& \cup && \{ && \textrm{\textbf{function }} fun(\tau_1, ..., \tau_n): \sigma \textrm{\textbf{ where }} \{ p = \langle t \rangle^r ~ | ~ ``p = t" \in eqns \} \\
& && | && `` \textrm{\textbf{function }} fun(\tau_1, ..., \tau_n): \sigma \textrm{\textbf{ where }} eqns " \in FDEF^{prg}_r \} 
\end{alignat*}

This definition is not quite legal, because the argument of $r^{data}$ possibly has equations with right-hand sides which cannot be correctly transformed by $r^{data}$. There a two different problems which can possibly arise, but which are also easily circumvented, as follows. Note that these points mirror those for \textsf{CoreDefunc}.
\begin{enumerate}
\item There are function calls to functions which are already refunctionalized (those in $F_r$); $r^{data}$ wants to transform them into destructor calls, which is neither possible nor necessary. The following technical trick circumvents this problem: For the sake of this usage of $r^{data}$, replace each function name $fun \in F_r$ (in a right-hand side term) with a constructor name $con_{fun}$ unique to $fun$, and set $\langle con_{fun} \rangle^r = fun$. Then $r^{data}$ will transform them back into the original function calls, which means that they ultimately are left unchanged, as desired.
\item There are destructor calls; the domain of $r^{data}$, the Data Fragment, doesn't allow them. To circumvent this problem, for the sake of this usage of $r^{data}$, replace such calls to a destructor named $des$ with function calls with a function name $fun_{des}$ unique to $des$, and set $\langle fun_{des} \rangle^d = des$. Then $r^{data}$ will transform them back into the original destructor calls, which means that they ultimately are left unchanged, as desired.
\end{enumerate}

Below, we define the transformation for terms $r$, which is only necessary due to the syntactic differences between function and destructor calls.

\begin{align*}
\langle x \rangle^r = x \\
\langle s.des(t_1, ..., t_n) \rangle^r = \langle s \rangle^r .des(\langle t_1 \rangle^r, ..., \langle t_n \rangle^r) \\
\langle fun(t_1, ..., t_n) \rangle^r = fun(\langle t_1 \rangle^r, ..., \langle t_n \rangle^r), \text{ if } fun \in F_r \\
\langle fun(t_1, ..., t_n) \rangle^r = \langle t_1 \rangle^r .\langle fun \rangle^r (\langle t_2 \rangle^r, ..., \langle t_n \rangle^r), \text{ if } fun \not\in F_r \\
\langle con(t_1, ..., t_n) \rangle^r = \langle con \rangle^r (\langle t_1 \rangle^r, ..., \langle t_n \rangle^r)
\end{align*}

\end{algorithm}

We consider important properties of the transformation.

\textbf{Bisimulation.} For \textsf{CoreRefunc}, strong bisimulation holds. The proof relies on properties of $r^{data}$. In the ``Bisimulation'' part of the previous subsection, we already outlined the main points: The domain of $r^{data}$ is the Data Fragment, and Rendel et al.s reduction relation for this fragment is the same as ours restricted to this fragment. In section 3 of their paper, Rendel et al. prove Lemma 5, which we have restated as Lemma 4.2.1. Using this, we prove strong bisimulation for \textsf{CoreRefunc}.

\begin{lemma}[Strong bisimulation for \textsf{CoreRefunc}]
\[
s \longrightarrow_{prg} t \iff \langle s \rangle^r \longrightarrow_{\langle prg \rangle^{\textsf{CoreRefunc}}} \langle t \rangle^r
\]

\begin{proof}
For the sake of this proof, we will neglect the syntactic differences between destructor and function calls. By the definition of \textsf{CoreRefunc}, it is
\[
\langle prg \rangle^{\textsf{CoreRefunc}} = \langle prg' \rangle^{r^{data}} \cup prg''
\]
such that $prg' \cup prg'' = prg$. By Lemma 4.2.1, the reduction relations pertaining to the programs $prg'$ and $\langle prg' \rangle^{r^{data}}$ are one and the same. The reduction relation pertaining to $prg$ is that of $prg'$ changed according to the equations of $prg''$, and the reduction relation pertaining to $\langle prg \rangle^{\textsf{CoreRefunc}}$ is also that of $prg'$ changed according to the equations of $prg''$. Thus these reduction relations are the same, which is equivalent to the statement of this lemma.
\end{proof}
\end{lemma}

\subsection{Example}

We continue with the example fragment of section 4.1.3, augmented with (co)data definitions and signatures.

\begin{lstlisting}

data P where
  con1(): P
  con2(): P

codata N1 where
  N1.des(): N2

codata N2 where
  N2.des1(P): P
  N2.des2(): P

function fun(N1): N1 where
  fun(x).des() = aux2(x)

function aux(P, N1): P where
  aux(con1(), x) = t1
  aux(con2(), x) = t2

function aux2(N1): N2 where
  aux2(x).des1(y) = aux(y, x)
  aux2(x).des2() = t3

\end{lstlisting}

Using the \textsf{CoreDefunc} algorithm on this yields the following program, by leaving the function definition for \texttt{aux} and the data definition unchanged, and applying the defunctionalization of Rendel et al. to the other definitions.

\begin{lstlisting}

data P where
  con1(): P
  con2(): P

data N1 where
  fun(N1): N1

data N2 where
  aux2(N1): N2

function aux(P, N1): P where
  aux(con1(), x) = t1
  aux(con2(), x) = t2

function des(N1): N2 where
  des(fun(x)) = aux2(x)

function des1(N2, P): P where
  des1(aux2(x), y) = aux(y, x)

function des2(N2): P where
  des2(aux2(x)) = t3

\end{lstlisting}

Using the \textsf{CoreRefunc} algorithm on the original program yields the following program, by leaving the function definitions for \texttt{fun} and \texttt{aux2} and the codata definitions unchanged, and applying the refunctionalization of Rendel et al. to the other definitions.

\begin{lstlisting}

codata P where
  P.aux(N1): P

codata N1 where
  N1.des(): N2

codata N2 where
  N2.des1(P): P
  N2.des2(): P

function fun(N1): N1 where
  fun(x).des() = aux2(x)

function aux2(N1): N2 where
  aux2(x).des1(y) = aux(y, x)
  aux2(x).des2() = t3

function con1() where
  con1().aux(x) = t1

function con2() where
  con2().aux(x) = t2

\end{lstlisting}

%%-- end under construction

...

Both de- and refunctionalization are made up of a couple of preprocessing steps, followed by the core de-/refunctionalization, which is essentially the two-way transformation from the paper of Rendel et al. Here, we give an overview of these steps for both transformations. In brackets, we name the step; this name will be used throughout the chapter. What can already be seen here is that steps $unmix$ and $elim\_cons\_from\_des$ are used for both transformations.

Automatic defunctionalization consists of the following steps:
\begin{enumerate}
\item Eliminate multiple destructors. ($elim\_multi\_des_d$)

\item Unmix function definitions. ($unmix$)

\item Eliminate constructors from destructor copatterns. ($elim\_cons\_from\_des$)

\item Core defunctionalization. ($d_{core}$)

\end{enumerate}

Automatic refunctionalization consists of the following steps:
\begin{enumerate}
\item Eliminate multiple destructors from copatterns containing constructors. ($elim\_multi\_des_r$)

\item Unmix function definitions. ($unmix$)

\item Eliminate constructors from destructor copatterns. ($elim\_cons\_from\_des$)

\item Eliminate multiple constructors. ($elim\_multi\_con$)

\item Core refunctionalization. ($r_{core}$)

\end{enumerate}

Both de- and refunctionalization require that all lhss of the program that is to be transformed are \textit{aligned}. We introduce this notion in the following section; an algorithm to achieve alignment can be found in appendix B.

The rest of the chapter is organized as follows. The first section introduces alignment and some notations used. In section 2, we define the preprocessing steps and show that they preserve the semantics of the program in a weak bisimulation; we also give their domains and ranges to show that they can be applied one after the other as laid out above. In section 3, we define the core de- and refunctionalization and prove strong bisimulation for them. Section 4 discusses related work.

\section{Preliminaries}

\subsection{Aligning patterns}

To prevent destructor extraction from introducing overlaps, it is necessary that, for all function definitions of the program, all lhss of the function definition \textit{align} -- and in fact, this property is already sufficient for this purpose. In short, for two copatterns $q_1, q_2$ to align means that any pattern directly under $q_1$ has the same specificity as its respective counterpart under $q_2$, if any. In this section, we first illustrate why the correctness of destructor extraction depends upon the alignment of the lhss. Then we formally define alignment and prove that alignment suffices to prevent the introduction of overlaps via destructor extraction. In appendix B, we give an algorithm for aligning copatterns.

When extracting destructors out of a program which has non-aligning lhss, the resulting program can have overlapping lhss even though the original program didn't. Consider the following example where the first and the third equations don't align (as well as the second and the third equation).

\begin{lstlisting}

fun().d1(c1()).d1() = t_1
fun().d1(c2()).d1() = t_2
fun().d1(x).d2() = t_3

\end{lstlisting}

Why don't these equations align? The third equation's left-hand side has a catch-all pattern (variable) in its first destructor call $d1$. The first destructor in the first and second equations' left-hand sides is also $d1$, but the patterns there are the constructors $c1()$ and $c2()$, respectively; they are more specific than the catch-all pattern.

If we target the first lhs for destructor extraction, its final destructor is extracted; the result of this is shown below (we omit the generated auxiliary function definition).

\begin{lstlisting}

fun().d1(c1()) = aux()
fun().d1(c2()).d1() = t_2
fun().d1(x).d2() = t_3

\end{lstlisting}

Now, the first and the third lhs overlap. For destructor extraction to work correctly, the catch-all pattern must be split before the extraction. The program fragment after this split is shown below.

\begin{lstlisting}

fun().d1(c1()).d1() = t_1
fun().d1(c2()).d1() = t_2
fun().d1(c1()).d2() = t_3
fun().d1(c2()).d2() = t_3

\end{lstlisting}

When we now target the first lhs for destructor extraction, this transformation also affects the third lhs because they have the same remains. Thus the extraction results in the equations shown below.

\begin{lstlisting}

fun().d1(c1()) = aux()
fun().d1(c2()).d1() = t_2
fun().d1(c1()) = aux()
fun().d1(c2()).d2() = t_3

\end{lstlisting}

The first and the third equation are identical and are thus present only once in the program, thus the program doesn't have overlapping lhss. The program that was transformed has only aligned lhss. In fact, to prevent destructor extraction from introducing overlaps it suffices that the transformed program has this property, as we will show further on.

Now, we formally define what it means for a (co-)pattern to be \textit{aligned} with another (co-)pattern.

\begin{definition}[Aligning patterns]
Two patterns $p_1, p_2$ of the same type align if they are (a) both variables, (b) both constructor calls of different constructors, or (c) constructor calls of the same constructor $con$ such that
\[
p_1 = con(p'_1, ..., p'_n), p_2 = con(p''_1, ..., p''_n),
\]
where, for all $i \in \{1, ..., n\}$, $p'_i$ and $p''_i$ align.
\end{definition}

\begin{definition}[Aligning copatterns]
Two copatterns $q_1, q_2$ of the same function definition, for a function $fun$, align if the respective arguments of their largest common destructor call chains align.
\end{definition}

Aligning patterns of a program before applying destructor extraction guarantees that the destructor extraction doesn't introduce overlaps. As shown in subsection 4.2.2, this follows from the following lemma.

\begin{lemma}
For any program $prg$ without overlapping lhss and where, for all of its function definitions $def$, all lhss of $def$ align with each other, the following holds: Whenever two prefixes of $prg$ overlap, one of them is a prefix of the other.

\begin{proof}
Suppose there are two lhss $q_1, q_2$ with prefixes $q^{\mathit{pref}}_1$ and $q^{\mathit{pref}}_2$, respectively, such that these prefixes overlap. Especially, this means that one of them -- w.l.o.g., $q^{\mathit{pref}}_1$ -- has an initial destructor chain $C$ that is identical to the entire destructor chain of the other, $q^{\mathit{pref}}_2$. Thus the entire destructor chain of $q^{\mathit{pref}}_2$ is a part of the largest common destructor chain of $q_1$ and $q_2$. It follows that all of the pattern arguments of the initial destructor chain $C$ of $q^{\mathit{pref}}_1$ align with the respective arguments of the destructor chain of $q^{\mathit{pref}}_2$. For the associated instances of these two destructor chains to overlap, it also needs to be the case that their respective pattern arguments overlap. Whenever two patterns overlap and align they are the same, thus $q^{\mathit{pref}}_2$ is a prefix of $q^{\mathit{pref}}_1$.
\end{proof}
\end{lemma}

\subsection{Notation}

We define the following conditional recursion combinator:
\[
    condrec(f, cond) :=
\begin{cases}
    condrec(f, cond) \circ f,& \text{if $cond$ holds} \\
   id,& \text{otherwise}
\end{cases}
\]

\section{Preprocessing steps}

Each preprocessing step is applied to each function definition individually. It is thus assumed that the function $fun$ of the function definition to be transformed is available to the transformation. For the complete step, simply transform all function definitions, in arbitrary order.

TODO: prove that each preprocessing terminates -- roughly, this is because (a) no auxiliary function definitions are transformed (they already have the desired form) and (b) each extraction strictly decreases, for at least one lhs, while leaving the others untouched, the following measure: the distance from the desired form (in number of destructors or constructors)

Roughly, each preprocessing step is defined as a recursive composition of one of three example extractions defined at the end of chapter 3 lifted to programs with $liftp$. More precisely, they all have definitions of the form
\[
condrec(liftp(extract \circ selector), cond),
\]
with the three parameters $cond$, $extract$, and $selector$, each of which can be explained as follows:
\begin{itemize}
\item $cond$: The continue condition. Further extractions are only carried out when this condition holds.

\item $extract$: The extraction transformation, as defined in chapter 3, with the targeted copattern, and possibly the targeted constructor (for $extract\_con$ only) left as argument(s).

\item $selector$: The function selecting the arguments for $extract$ from the function definition of $fun$. In its stead, a function for every argument is defined: $q\_selector$ to select the targeted copattern, and, only for $extract\_con$, $con\_selector$ tos slect the targeted constructor.
\end{itemize}
The three parameters $cond$, $extract$, and $selector$ uniquely determine the desired preprocessing step, thus all of those steps will be defined by only defining them. In any case $extract$ is chosen to be one of the example extractions from section 3.5, that is, either $extract\_des$, $extract\_con$, or $extract\_patterns$.

For each preprocessing, to show that a kind of weak bisimulation holds it suffices to show that, at no point in the recursive extraction, overlapping lhss are generated. From chapter 3, we know that the in there described kind of weak bisimulation holds for any extraction transformation as defined in chapter 3, as long as the transformed program doesn't have overlapping lhss. Both steps are defined as recursive compositions of $extract\_des$, thus we can combine the bisimulation statements for $extract\_des$ to obtain a kind of weak bisimulation for the entire preprocessing step, provided that overlapping lhss are never generated in the recursive process.

\subsection{Eliminate multiple destructors}

In this section, we define both $elim\_multi\_des_d$ and $elim\_multi\_des_r$, by defining their common parameters $q\_selector$ and $extract$ and their respective continue conditions $cond_d$ and $cond_r$.

\begin{itemize}
\item $q\_selector$: Selects an lhs with a maximal number of destructors (but is otherwise arbitrary).

\item $extract := extract\_des$ (as defined in chapter 3).

\item $cond_d$: Continue when the function definition still contains multiple destructor lhss.

\item $cond_r$: Continue when the function definition still contains multiple destructor lhss with constructors.
\end{itemize}

Now, we show that, at no point in the recursive extraction, overlapping lhss are generated. We first show this under the (yet) unproven assumption that at all points in the recursive process, all lhss of the program align -- or equivalently, that destructor extraction preserves alignment. Then we show that this assumption about alignment holds (assuming, as stated at the start of the chapter, that alignment holds for the original program's lhss).

\textbf{Absence of overlaps.} Consider a singular destructor extraction. By Proposition 3.4.1, we need only compare equations taken over unchanged and the equation $\epsilon$ generated by the extraction. By Lemma 4.1.1, since all lhss of the input program align, whenever two prefixes of lhss of the original program overlap, one of them is a prefix of the other. $q_\epsilon$ is a prefix of an lhs $q_r$ of the original program; if it overlapped with a $q_{r'}$ taken over unchanged then $q_{r'}$ (a prefix of itself) and the prefix $q_\epsilon$ of $q_r$ overlapped. But then, by Lemma 4.1.1, either (a) $q_\epsilon$ is a prefix of $q_{r'}$ or (b) $q_{r'}$ is a prefix of $q_\epsilon$. In case (a) it is either $q_\epsilon = q_{r'}$, but then $q_{r'}$ and $q_r$ overlapped contrary to assumption, or, since $q_{r'}$ doesn't have more destructors than $q_r$, $q_{r'}$ is identical to $q_r$ except for the last destructor call, but then $q_{r'}$ would also be targeted by the destructor extraction, contrary to assumption. In case (b) $q_{r'}$ is a prefix of $q_r$ and thus they overlap, again contrary to assumption.

\textbf{Alignment.} Consider a singular destructor extraction, and assume that its input program's lhss all align with each other. The only lhss of the program which have changed are (a) those in the function definition of $fun$ which have lost a destructor, and (b) those newly added with the equations of the auxiliary function. The lhss (a) still align with all other lhss of the function definition, since removing destructor only possibly removed some counterparts of patterns under other lhss. The lhss (b) all have the form $aux(\underline{x}).des(\overline{p})$; the variables align, and so do the arguments of the destructors since they already did before the extraction, as destructors were only extracted out of lhss with identical destructor chains.

\subsection{Unmix}

In this section, we define $unmix$, by defining its parameters $q\_selector$, $extract$, and $cond$.

\begin{itemize}
\item $q\_selector$: Selects an lhs with a maximal number of destructors (but is otherwise arbitrary).

\item $extract := extract\_des$ (as defined in section 3.5.1).

\item $cond$: Continue when the function definition still is mixed.
\end{itemize}

Overlapping lhss are never generated, for the same reasons we gave for $elim\_multi\_des_d$ and $elim\_multi\_des_r$ in section 4.2.1.

\subsection{Eliminate constructors from destructor copatterns}

In this section, we define $elim\_cons\_from\_des$, by defining its parameters $q\_selector$, $extract$, and $cond$.

\begin{itemize}
\item $q\_selector$: Selects an arbitrary lhs.

\item $extract := extract\_patterns$ (as defined in section 3.5.3).

\item $cond$: Continue when the function definition still has lhss with constructors.
\end{itemize}

Now, we show that, at no point in the recursive extraction, overlapping lhss are generated. We consider a singular extraction with $extract\_patterns$. By Proposition 3.4.1, we need only compare equations taken over unchanged and the equation $\epsilon$ generated by the extraction. The extraction $extract\_patterns$ is only ever applied to single-destructor copatterns and transforms them to single-destructor copatterns without any constructors. It is also only ever used in unmixed function definitions where all lhss have exactly one destructor. If the destructor of such an lhs $q$ is different from that of $q_\epsilon$, they cannot overlap. But if the destructor of $q$ is the same as that of $q_\epsilon$, and thus the same as that of $q_r$, the extraction result is the same for both $q_r$ and $q$. Thus, by the definition of extraction functions, the equations of $q_r$ and $q$ are the same.

\subsection{Eliminate multiple constructors}

In this section, we define $elim\_multi\_con$, by defining its parameters $q\_selector$, $extract$, and $cond$.

\begin{itemize}
\item $q\_selector$: Selects an arbitrary lhs.

\item $con\_selector$: Selects an arbitrary constructor that is not a left-outer constructor, i.e., not $con_1$ in $fun(con_1(\overline{p}), \overline{p'})$.

\item $extract := extract\_con$ (as defined in section 3.5.2).

\item $cond$: Continue when there still is an lhs that doesn't have the form $fun(con(\overline{x}), \overline{y})$ or $fun(\overline{x})$.
\end{itemize}

Now, we show that, at no point in the recursive extraction, overlapping lhss are generated. We consider a singular extraction with $extract\_con$. TODO

\section{Core de- and refunctionalization}

\subsection{Core defunctionalization}

After the preprocessing steps, the only thing that remains is to apply the defunctionalization for the Codata Fragment of Uroboro, as developed by Rendel et al., to the not yet defunctionalized parts of the program. It can be applied to these parts because the preprocessing steps guarantee that they are in the Codata Fragment. Call the defunctionalization for the Data Fragment of Uroboro $d^{codata}$; the core defunctionalization for programs is defined as follows below.

\begin{alignat*}{3}
\langle prg \rangle^{d_{core}} & = ~&& \langle && \{ def \in prg ~ | ~ def \textrm{ is codata def. or} \\ & && &&\quad \textrm{ function def. with equations } eqns \neq \emptyset: \forall e \in eqns: e \textrm{ has destr. pattern } \} \rangle^{d^{codata}} \\
& \cup && \{ && \textrm{\textbf{data }} ... ~ | ~ `` \textrm{\textbf{data }} ... " \in prg \} \\
& \cup && \{ && \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} \{ p = \langle t \rangle^d ~ | ~ "p = t" \in eqns \} \\
& && | && `` \textrm{\textbf{function }} fun(\sigma, \tau_1, ..., \tau_k): \tau \textrm{\textbf{ where }} eqns " \in prg \textrm{ with } \forall e \in eqns: e \textrm{ has hole pattern}\} 
\end{alignat*}

Technical note on constructor subsumption:

The input of $d^{codata}$ in the definition above is actually not in its domain. This is because it can contain constructor calls. The following technical trick allows to transform such inputs as well: For the sake of $d^{codata}$, subsume constructor names under function names (as if they were from the same syntactic domain). After the transformation, since names aren't changed (or when name changes are desired, the original name can still be retrieved), the subsumed constructor names (or their equivalents after a name change) are once again considered constructor names (from the original syntactic domain).

Defunctionalizing terms: \\
$\langle x \rangle^d = x$ \\
$\langle s.des(t_1, ..., t_n) \rangle^d = \langle des \rangle^d (\langle s \rangle^d, \langle t_1 \rangle^d, ..., \langle t_n \rangle^d)$ \\
$\langle fun(t_1, ..., t_n) \rangle^d = \langle fun \rangle^d (\langle t_1 \rangle^d, ..., \langle t_n \rangle^d)$ \\
$\langle con(t_1, ..., t_n) \rangle^d = con(\langle t_1 \rangle^d, ..., \langle t_n \rangle^d)$ \\

\subsubsection{Proof of strong bisimulation}

For $d_{core}$, strong bisimulation holds. The proof relies on properties of $d^{codata}$. As stated in section 2.3.1, the authors' notion of reducibility is the same than that of this work when restricted to the domain of $d^{codata}$, the Codata Fragment.

In section 3, Rendel et al. prove Lemma 5, which in terms of this work can be stated as follows (possible since the reducibility notions are identical):

$s \longrightarrow_{prg} t \iff \langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle$ for all input terms $s,t$ of $\langle \cdot \rangle$ (*)

Here, the angular brackets can stand for either of their transformations, the refunctionalization $r^{data}$ and the defunctionalization $d^{codata}$. Statement (*) means that strong bisimulation holds for $d^{codata}$.

Using (*), it will now be shown that strong bisimulation holds for $d_{core}$.

\begin{proof}[Proof of strong bisimulation for $d_{core}$] ~

$`` \Rightarrow "$: By induction on the structure of $\mathcal{D}$.

\begin{enumerate}
\item \textbf{``Subst" case}:

\begin{prooftree}
\AxiomC{$\mathcal{D}_{\textrm{PM}}$}
\UnaryInfC{$s =^? q \searrow \sigma$ with $(q, s') \in \textrm{Rules}(prg)$}
\UnaryInfC{$s \longrightarrow s'[\sigma]$}
\end{prooftree}

with $s'[\sigma] = t$; the immediate subterms of $s$ are values; $\mathcal{D}_{\textrm{PM}}$ is a derivation of the pattern matching. This transformation changes input terms, thus $\langle s \rangle = \langle s \rangle^d$, $\langle t \rangle = \langle t \rangle^d$. $d$ is the defunctionalization of terms as defined above. This defunctionalization of terms is also, for all input terms from the fragment, identical to that of the Codata Fragment.

\begin{itemize}

\item \underline{Case 1}: $q$ is hole pattern:

Then the function definition that contains $`` q = s' "$ contains only equations where the left-hand side is a hole pattern (other cases are excluded by the relevant input fragment for $d_{core}$). Such equations (and indeed the function definitions) are left unchanged by $d_{core}$ except for defunctionalizing the right-hand term, as can be seen directly in the definition of $d_{core}$ (last set in the highest-level union). Thus Rules($\langle prg \rangle$) contains $(q, \langle s' \rangle)$.

By inversion, we have from $s =^? q \searrow \sigma$ that $s$ has the form $fun(v_1, ..., v_n)$ for some values $v_1, ..., v_n$, thus $\langle s \rangle = fun(\langle v_1 \rangle, ..., \langle v_n \rangle)$. By inversion for values, we have that each $v_i$ is either a constructor application or a value of codata type. If it is a value of codata type, by inversion on pattern matching, the relevant subpattern of $q$ can only be a variable, thus it is also matched by $\langle v_i \rangle$. If it is a constructor application, the relevant subpattern of $q$ is either a variable, and the same holds, or it is a constructor pattern, and by recursively descending into its subpatterns we still get that $\langle v_i \rangle = con(\langle v^1_i \rangle, ..., \langle v^m_n \rangle)$ matches against the subpattern of $q$.

By carrying the substitutions returned from the matchings along in the above recursive argument, we get a substitution $\sigma'$ such that $\langle s \rangle =^? q \searrow \sigma'$ and, by distributing over $\langle s' \rangle$, $\langle s' \rangle [\sigma'] = \langle s'[\sigma] \rangle = \langle t \rangle$. It follows that $\langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle$.

\item \underline{Case 2}: $q = fun(p_1, ..., p_n).des(p'_1, ..., p'_k)$:

Then the function definition that contains $`` q = s' "$ contains only equations where the left-hand side is a destructor pattern (other cases are excluded by the relevant input fragment for $d_{core}$). Thus $s$ reduces to $t$ already with respect to the part of the program that is passed to $d^{codata}$, as specified in the definition of $d_{core}$. Let this part, amended by the ``constructor subsumption" noted for the definition of $d_{core}$, be $prg'$; it is: $s \longrightarrow_{prg'} t$

By (*) we would have

\begin{equation*}
s \longrightarrow_{prg'} t \iff \langle s \rangle \longrightarrow_{\langle prg' \rangle^{d^{codata}}} \langle t \rangle,
\end{equation*}

were $prg'$ a well-typed program with copattern coverage for all subterms of $s$. 

For the coverage, bear in mind that the equation $`` q = s' "$ enabling the reduction of $s$ by the ``Subst" rule is part of $prg'$ by the precondition of Case 2. As $s$ matches against $q$, copattern coverage for $s$ is trivially fulfilled in $prg'$. The immediate subterms of $s$ are values with respect to $prg$ and, by inversion, their immediate subterms and so forth, which especially means that there is no rule in $prg$ against which they match. But $prg$ has copattern coverage for such a subterm (TODO: make this a general precondition) and there is already no rule for it in $prg$. It follows that $prg'$ still has copattern coverage for the subterm even though there is no rule for it in $prg'$. This is because, either (1) the subterm is a destructor call, then it can only be covered by destructor copatterns (as it matches against a destructor copattern) and those only occur within $prg'$, or (2) it is a constructor call, which doesn't need to be matched for coverage. It can't be a function call, since these can only be covered by directly matching the call, which isn't the case even in $prg$, for which coverage is assumed. Thus coverage holds for $prg'$.

For well-typedness, simply treat the missing types temporarily, that is, for the sake of (*), as codata types. This is no problem for the restriction to the domain of $d^{codata}$, since such types could be introduced inside the Codata Fragment with codata definitions. To be more precise, empty function definitions can be added for missing ones and empty codata definitions for missing types, and removed again after using (*), without adding or removing possible reductions, respectively. All in all, we have by (*):
\begin{equation*}
s \longrightarrow_{prg'} t \iff \langle s \rangle \longrightarrow_{\langle prg' \rangle^{d^{codata}}} \langle t \rangle
\end{equation*}

But this program $\langle prg' \rangle^{d^{codata}}$ is a subset of $\langle prg \rangle$, as can be seen in the definition of $d_{core}$. This implies the desired $\langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle$.

\end{itemize}

Other cases are excluded by the relevant input fragment.

\item \textbf{``Cong" case}:

\begin{prooftree}
\AxiomC{$s' \longrightarrow t'$}
\RightLabel{Cong}
\UnaryInfC{$\mathcal{E}[s'] \longrightarrow \mathcal{E}[t']$}
\end{prooftree}

with $\mathcal{E}[s'] = s$ and $\mathcal{E}[t'] = t$.

By the induction hypothesis we have $\langle s' \rangle \longrightarrow_{\langle prg \rangle} \langle t' \rangle$. Let $\langle \mathcal{E} \rangle$ denote the transformation of $\mathcal{E}$, defined analogously to the transformation of terms by transforming the terms in $\mathcal{E}$ and by setting $\langle [] \rangle = []$. By applying the congruence rule we get $\langle \mathcal{E} \rangle[\langle s' \rangle] \longrightarrow_{\langle prg \rangle} \langle \mathcal{E} \rangle[\langle t' \rangle]$. It is clear that $\langle \mathcal{E} \rangle[\langle s' \rangle] = \langle \mathcal{E}[s'] \rangle = \langle s \rangle$ and $\langle \mathcal{E} \rangle[\langle t' \rangle] = \langle \mathcal{E}[t'] \rangle = \langle t \rangle$.

\end{enumerate}

$`` \Leftarrow "$: By induction on the structure of $\mathcal{D}$.

\begin{enumerate}
\item \textbf{``Subst" case}:

\begin{prooftree}
\AxiomC{$\mathcal{D}_{\textrm{PM}}$}
\UnaryInfC{$\langle s \rangle =^? q \searrow \sigma$ with $(q, s') \in \textrm{Rules}(\langle prg \rangle)$}
\UnaryInfC{$\langle s \rangle \longrightarrow_{\langle prg \rangle} s'[\sigma]$}
\end{prooftree}

with $s'[\sigma] = \langle t \rangle$; the immediate subterms of $\langle s \rangle$ are values; $\mathcal{D}_{\textrm{PM}}$ is a derivation of the pattern matching. This transformation changes input terms, thus $\langle s \rangle = \langle s \rangle^d$, $\langle t \rangle = \langle t \rangle^d$. $d$ is the defunctionalization of terms as defined above. This defunctionalization of terms is also, for all input terms from the fragment, identical to that of the Codata Fragment.

The equation $`` q = s' "$ can either be contained in that part of $\langle prg \rangle$ that results from the application of $d^{codata}$ to the relevant part of $prg$, as specified in the definition of $d''$, or it can be in the other part of $\langle prg \rangle$. As can be seen in the definition of $d''$, this other part is taken over unchanged from $prg$ except for defunctionalizing the right-hand terms. Thus for an equation $`` q = s' "$ from this part, the equation $`` q = s'' "$ with $s' = \langle s'' \rangle$ is present in $prg$. For such an equation, $q$ has hole pattern. It can then be easily seen that $s =^? q \searrow \sigma'$ for a $\sigma'$ with $s''[\sigma'] = t$ by an argument analogous to that of $`` \Rightarrow "$, ``Subst" case, Case 1.

Now, suppose that $`` q = s' "$ is contained in the part of $\langle prg \rangle$ that results from the application of $d^{codata}$ to the relevant part $prg' \subseteq prg$. Thus $\langle s \rangle \longrightarrow_{\langle prg' \rangle^{d^{codata}}} \langle t \rangle$.

By (*) we would have

\begin{equation*}
\langle s \rangle \longrightarrow_{\langle prg' \rangle^{d^{codata}}} \langle t \rangle \iff s \longrightarrow_{prg'} t,
\end{equation*}

were $prg'$ a well-typed program with copattern coverage for all subterms of $s$. Both of those properties can be shown or simulated similarly to the way they are in the $`` \Rightarrow "$ part.

But it is $prg' \subseteq prg$, as can be seen in the definition of $d''$. This implies the desired $s \longrightarrow_{prg} t$.

\item \textbf{``Cong" case}:

\begin{prooftree}
\AxiomC{$s' \longrightarrow_{\langle prg \rangle} t'$}
\RightLabel{Cong}
\UnaryInfC{$\mathcal{E}[s'] \longrightarrow \mathcal{E}[t']$}
\end{prooftree}

with $\mathcal{E}[s'] = \langle s \rangle$ and $\mathcal{E}[t'] = \langle t \rangle$.

By the induction hypothesis we have $s'' \longrightarrow_{prg} t''$ with $s' = \langle s'' \rangle$, $t' = \langle t'' \rangle$. Let $\langle \mathcal{E} \rangle$ denote the transformation of $\mathcal{E}$ (defined as in the $`` \Rightarrow "$ part). Apply the congruence rule to get $\mathcal{E}'[s''] \longrightarrow_{prg} \mathcal{E}'[t'']$ with $\mathcal{E} = \langle \mathcal{E}' \rangle$. That is, $\mathcal{E}'$ is the result of applying the inverse of $\langle \cdot \rangle$ to $\mathcal{E}$, which is possible, since, for instance, $\mathcal{E}[s'] = \langle s \rangle$. It is $\langle \mathcal{E}'[s''] \rangle = \langle \mathcal{E}' \rangle[\langle s'' \rangle] = \mathcal{E}[s'] = \langle s \rangle$ and $\langle \mathcal{E}'[t''] \rangle = \langle \mathcal{E}' \rangle[\langle t'' \rangle] = \mathcal{E}[t'] = \langle t \rangle$ and thus we have the desired $s \longrightarrow_{prg} t$.
\end{enumerate}

\end{proof}

\subsection{Core refunctionalization}

This is defined analogously to core defunctionalization, by applying the refunctionalization for the Data Fragment of Uroboro to the not yet refunctionalized parts of the program. It can be applied to these parts because the preprocessing steps guarantee that they are in the Data Fragment. Call the refunctionalization for the Data Fragment of Uroboro $r^{data}$; the core refunctionalization for programs is defined as follows below.

First, a technical note: As $r_{core}$ doesn't allow destructor terms in its inputs, they have to be converted beforehand. This conversion is the same as that of $r$ for terms below, restricted to destructor terms. Call this conversion lifted to programs (in the way that all destructor terms on right-hand sides or as subterms of them are converted) $des\_conv$.

\begin{alignat*}{3}
\langle prg \rangle^{r_{core}} & = ~&& \langle \langle && \{ def \in prg ~ | ~ def \textrm{ is data def. or} \\ & && &&\quad \textrm{ function def. with equations } eqns \neq \emptyset: \forall e \in eqns: e \textrm{ has no destr. pattern}, \\
& && &&\qquad \textrm{the first argument of the lhs isn't a variable } \} \rangle^{des\_conv} \rangle^{r_{core}} \\
& \cup && \{ && \textrm{\textbf{codata }} ... ~ | ~ `` \textrm{\textbf{codata }} ... " \in prg \} \\
& \cup && \{ && \textrm{\textbf{function }} fun(\tau_1, ..., \tau_n): \sigma \textrm{\textbf{ where }} \{ p = \langle t, prg \rangle^r ~ | ~ "p = t" \in eqns \} \\
& && | && `` \textrm{\textbf{function }} fun(\tau_1, ..., \tau_n): \sigma \textrm{\textbf{ where }} eqns " \in prg \textrm{ with } \forall e \in eqns: e \textrm{ has destr. pattern} \\
& && &&\quad \textrm{or where } n = 0 \textrm{ or where the first argument of the lhs is a variable} \} 
\end{alignat*}

Along with the transformation for programs, a transformation of terms is necessary, which is a conservative extension of $r^{data}$ for programs. For this, write $r$ short for $r_{core}$ \\
$\langle x, prg \rangle^r = x$ \\
$\langle s.des(t_1, ..., t_n), prg \rangle^r = \langle s, prg \rangle^r .des(\langle t_1, prg \rangle^r, ..., \langle t_n, prg \rangle^r)$ \\
$\langle fun(t_1, ..., t_n), prg \rangle^r = fun(\langle t_1, prg \rangle^r, ..., \langle t_n, prg \rangle^r)$, \\
if ``\textbf{function} $fun(\tau_n, ..., \tau_n): \sigma$ \textbf{where} $eqns$" $\in prg$  with $\forall e \in eqns: e$ has destructor pattern or where $n = 0$ or where the first argument of the lhs is a variable \\
$\langle fun(t_1, ..., t_n), prg \rangle^r = \langle t_1, prg \rangle^r .\langle fun, prg \rangle^r (\langle t_2, prg \rangle^r, ..., \langle t_n, prg \rangle^r)$, \\
otherwise \\
$\langle con(t_1, ..., t_n), prg \rangle^r = \langle con, prg \rangle^r (\langle t_1, prg \rangle^r, ..., \langle t_n, prg \rangle^r)$ \\

Note that the case distinction above is only necessary because of the special syntax for destructors ($q(...).des(...)$ instead of $des(..., ...)$).

\subsubsection{Proof of strong bisimulation}

For $r_{core}$, strong bisimulation holds. The proof relies on properties of $r^{data}$. As stated in section 2.3.1, the authors' notion of reducibility is the same than that of this work when restricted to the domain of $r^{data}$, the Data Fragment.

In section 3, Rendel et al. prove Lemma 5, which in terms of this work can be stated as follows (possible since the reducibility notions are identical):

$s \longrightarrow_{prg} t \iff \langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle$ for all input terms $s,t$ of $\langle \cdot \rangle$ (*)

Here, the angular brackets can stand for either of their transformations, the refunctionalization $r^{data}$ and the defunctionalization $d^{codata}$. Statement (*) means that strong bisimulation holds for $r^{data}$.

Using (*), it will now be shown that strong bisimulation holds for $r_{core}$.

\begin{proof}[Proof of strong bisimulation for $r_{core}$] ~

$`` \Rightarrow "$: By induction on the structure of $\mathcal{D}$.

\begin{enumerate}
\item \textbf{``Subst" case}:

\begin{prooftree}
\AxiomC{$\mathcal{D}_{\textrm{PM}}$}
\UnaryInfC{$s =^? q \searrow \sigma$ with $(q, s') \in \textrm{Rules}(prg)$}
\UnaryInfC{$s \longrightarrow s'[\sigma]$}
\end{prooftree}

with $s'[\sigma] = t$; the immediate subterms of $s$ are values; $\mathcal{D}_{\textrm{PM}}$ is a derivation of the pattern matching. This transformation changes input terms, thus $\langle s \rangle = \langle s \rangle^r$, $\langle t \rangle = \langle t \rangle^r$. $r$ is the refunctionalization of terms as defined above (it is omitted that $prg$ is passed to $r$ as well). This refunctionalization of terms is also, for all input terms from the fragment, identical to that of the Data Fragment.

\begin{itemize}

\item \underline{Case 1}: $q$ is destructor pattern:

Then the function definition that contains $`` q = s' "$ contains only equations where the left-hand side is a destructor pattern (other cases are excluded by the relevant input fragment for $r_{core}$). Such equations (and indeed the function definitions) are left unchanged by $r_{core}$ except for refunctionalizing the right-hand term, as can be seen directly in the definition of $r_{core}$ (last set in the highest-level union). Thus Rules($\langle prg \rangle$) contains $(q, \langle s' \rangle)$.

From here, the argument proceeds analogously to that of $`` \Rightarrow "$, ``Subst" case, Case 1, in the proof for $d_{core}$.

\item \underline{Case 2}: $q$ is hole pattern without arguments or where the first argument is a variable:

Then the equation is left unchanged by $r_{core}$ except for refunctionalizing the right-hand term, as can be seen directly in the definition of $r_{core}$ (last set in the highest-level union). Proceed as in Case 1.

\item \underline{Case 3}: $q$ is hole pattern and has a first argument which is a constructor pattern:

Then the function definition that contains $`` q = s' "$ contains only equations where the left-hand side is a hole pattern (other cases are excluded by the relevant input fragment for $r_{core}$), and it has a first argument with data type. Thus $s$ reduces to $t$ already with respect to the part of the program that is passed to $des\_conv$, and then the result of this to $r^{data}$, as specified in the definition of $r_{core}$. Let the part passed to $des\_conv$ be $prg'$; it is: $s \longrightarrow_{prg'} t$.

By (*) we have

\begin{equation*}
s \longrightarrow_{prg'} t \iff \langle s \rangle \longrightarrow_{\langle prg' \rangle^{r^{data}}} \langle t \rangle,
\end{equation*}

But this program $\langle prg' \rangle^{r^{data}}$ is a subset of $\langle prg \rangle$, as can be seen in the definition of $r_{core}$. Thus we have the desired $\langle s \rangle \longrightarrow_{\langle prg \rangle} \langle t \rangle$.

\end{itemize}

\item \textbf{``Cong" case}:

The argument here is identical to that of this case of this direction in the proof for $d_{core}$.

\end{enumerate}

$`` \Leftarrow "$: By induction on the structure of $\mathcal{D}$.

\begin{enumerate}
\item \textbf{``Subst" case}:

\begin{prooftree}
\AxiomC{$\mathcal{D}_{\textrm{PM}}$}
\UnaryInfC{$\langle s \rangle =^? q \searrow \sigma$ with $(q, s') \in \textrm{Rules}(\langle prg \rangle)$}
\UnaryInfC{$\langle s \rangle \longrightarrow_{\langle prg \rangle} s'[\sigma]$}
\end{prooftree}

with $s'[\sigma] = \langle t \rangle$; the immediate subterms of $\langle s \rangle$ are values; $\mathcal{D}_{\textrm{PM}}$ is a derivation of the pattern matching. This transformation changes input terms, thus $\langle s \rangle = \langle s \rangle^r$, $\langle t \rangle = \langle t \rangle^r$. $r$ is the refunctionalization of terms defined above. This refunctionalization of terms is also, for all input terms from the fragment, identical to that of the Data Fragment.

The equation $`` q = s' "$ can either be contained in that part of $\langle prg \rangle$ that results from the application of $des\_conv$ and then $r^{data}$ to the relevant part of $prg$, as specified in the definition of $r_{core}$, or it can be in the other part of $\langle prg \rangle$. As can be seen in the definition of $r_{core}$, this other part is taken over unchanged from $prg$ except for refunctionalizing the right-hand terms. Thus for an equation $`` q = s' "$ from this part, the equation $`` q = s'' "$ with $s' = \langle s'' \rangle$ is present in $prg$. For such an equation, $q$ has hole pattern. It can then be easily seen that $s =^? q \searrow \sigma'$ for a $\sigma'$ with $s''[\sigma'] = t$ by an argument analogous to that of $`` \Rightarrow "$, ``Subst" case, Case 1, in the proof for $d_{core}$.

Now, suppose that $`` q = s' "$ is contained in the part of $\langle prg \rangle$ that results from the application of $des\_conv$ and then $r^{data}$ to the relevant part $prg' \subseteq prg$. Thus $\langle s \rangle \longrightarrow_{\langle \langle prg' \rangle^{des\_conv} \rangle^{r^{data}}} \langle t \rangle$.

By (*) we have

\begin{equation*}
\langle s \rangle \longrightarrow_{\langle \langle prg' \rangle^{des\_conv} \rangle^{r^{data}}} \langle t \rangle \iff s \longrightarrow_{\langle prg' \rangle^{des\_conv}} t.
\end{equation*}

In the result of $des\_conv$, no new matching left-hand sides are added. That is, $prg'$ contains at least all the matching left-hand sides that $\langle prg' \rangle^{des\_conv}$ has. Thus any reduction that is possible with respect to $\langle prg' \rangle^{des\_conv}$ is already possible with respect to $prg'$.

But it is $prg' \subseteq prg$, as can be seen in the definition of $r_{core}$. This implies the desired $s \longrightarrow_{prg} t$.
\end{enumerate}

\item \textbf{``Cong" case}:

The argument here is identical to that of this case of this direction in the proof for $d_{core}$.

\end{proof}

%\section{Simplifying copatterns (Alternative approach to de- and refunc.}
%
%Both de- and refunctionalization are made up of two major parts:
%\begin{enumerate}
%\item First, destructor and constructor extractions alternate to transform the program into a form which can be used by the second part.
%
%\item This second part is the core de-/refunctionalization, which is essentially the two-way transformation from the paper of Rendel et al.
%\end{enumerate}
%
%The \textit{simplifying} part of the defunctionalization transformation is made up of destructor and constructor extraction steps and stops when the program is in the input fragment of core defunctionalization, described below. The simplifying part of the refunctionalization transformation is defined like that, with the only difference being that it stops when the program is in the input fragment of core refunctionalization, also described below.
%
%This simplification is done individually for each function definition, the order in which the function definitions are transformed is unimportant. For one function definition $def$, the algorithm is defined below.
%
%\[
%  \langle prg \rangle^{simplify(def)}=\begin{cases}
%               prg, &\text{ if $def$ is in the desired fragment}\\
%               \langle prg \rangle^{simplify\_step(def)} \rangle^{simplify(def)}, &\text{ otherwise}
%            \end{cases}
%\]
%
%\[
%  \langle prg \rangle^{simplify\_step(def)}=\begin{cases}
%               \langle prg \rangle^{liftp(des\_extract(q^{max}_{def}))}, \\
%               \qquad\text{ if } \langle prg \rangle^{liftp(con_{n_{def}}\_extract(q^{max}_{def}))} \text{ has overlaps}\\
%               \langle prg \rangle^{liftp(con_{n_{def}}\_extract(q^{max}_{def}))},\\
%               \qquad\text{ otherwise}
%            \end{cases}
%\]
%
%with
%
%\[
%q^{max}_{def} = \textrm{max}_{\# con.} \textrm{max}_{\# des.} \{q ~ | ~ q \text{ is lhs in $def$ } \}
%\]
%
%and $n_{def}$ the number of the inner-most constructor in $q^{max}_{def}$ which has a variable ``in its place'' in another lhs of $def$. For a pattern $p$ of a copattern $q$ to be ``in the place'' of another pattern $p'$ in another copattern $q'$ means:
%\begin{itemize}
%\item When $p$ is a subterm of the $n$-th pattern immediately under $q$, then $p'$ is a subterm of the $n$-th pattern immediately under $q$,
%
%\item the same for the $m$-th pattern immediately under the $n$-th patterns if $p$ and/or $p'$ aren't the $n$-th patterns themselves,
%
%\item and so on recursively.
%\end{itemize}
%
%As can be seen in the definition of $simplify\_step$, the algorithm switches from destructor to constructor extraction whenever constructor extraction would produce overlapping lhss. This actually prevents overlaps, because, whenever constructor extraction would produce overlaps, destructor extraction doesn't, as will be shown below.
%
%Clearly, this also means that the algorithm eventually arrives at a function definition without any destructors and without any constructors, unless it stops before that, thus ensuring that the desired fragment will be reached in any case.
%
%\subsection{Bisimulation}
%
%Two properties are desired for this algorithm: some kind of bisimulation, and that no overlapping lhss are generated. Since the algorithm is made up only of $des\_extract$ and $con\_extract$ steps, these properties follow from their respective properties.
%
%When overlapping lhss are absent in the transformed program, we have the kind of weak bisimulation as established by Proposition 2.3.1 for both $des\_extract$ and $con\_extract$, irrespective of which lhs is targeted. That overlaps aren't generated by one such step will be shown in the following subsection.
%
%\subsection{Absence of overlaps}
%
%By Proposition 2.4.1, for both $des\_extract$ and $con\_extract$, it suffices to show that $q_\epsilon$ doesn't overlap with unchanged lhss. This doesn't hold for arbitrary targeted lhss; in order to avoid generating overlaps, $simplify\_step$ was defined as above. It is now shown that this definition really prevents overlaps in the resulting program of one such step. As pointed out above, because of the definition of $simplify\_step$, it suffices to show that destructor extraction doesn't produce overlaps whenever constructor extraction does.
%
%When constructor extraction leads to overlaps, ...
