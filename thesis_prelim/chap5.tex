% !TEX root = main.tex
\chapter{Implementation and applications}
\label{ch:impl}

We have implemented the application of extractions to programs and, based on this, the de- and refunctionalization algorithms in Haskell.\footnote{The source code is found at \url{http://github.com/lordxist/uroboro-transformations}.} Using this implementation, we have automatically transformed an example for defunctionalization and one for refunctionalization known from the literature.

In \autoref{sec:impl}, we describe our implementation, focusing on the organization of its modules and design choices not dictated by the formal description of the algorithms. We then consider applications of our transformations to examples from the literature. In \autoref{ssec:mci} we show how Reynolds' classic example, the meta-circular interpreter for the lambda calculus\citep{reynolds72definitional}, can be translated to Uroboro and then automatically defunctionalized using our implementation, In \autoref{ssec:dyck} we apply our transformations to a refunctionalization example from \citet{danvy09refunctionalization}.

\section{Implementation}
\label{sec:impl}

We have implemented the various algorithms formally described in this work as a Haskell library; we have added some simple executables for defunctionalization, refunctionalization, core defunctionalization, core refunctionalization, unnesting for defunctionalization, and unnesting for refunctionalization. Our library makes use of the representations for Uroboro parse trees and typechecked programs developed by Tobias Weber.\footnote{Our Haskell package \texttt{uroboro-transformations} depends on our fork of his project, found at \url{http://github.com/lordxist/uroboro}. His original repository is found at \url{http://github.com/tewe/uroboro}.} The library is organized in several modules, which we now detail.

At the heart is module \texttt{Extraction}, which exposes a function \texttt{applyExtraction} that allows a user to apply an extraction to a function definition, yielding the changed function definition and the auxiliary function definition. The user has to specify the extraction to be applied; for this, a data type \texttt{ExtractionSpec} is provided, which has fields for the extraction target, the lens underlying the extraction, and the whole program the function definition is a part of. The last of these is needed to avoid name collisions when generating the name for the auxiliary function. The lens underlying the extraction is captured by a data type \texttt{ExtractionLens} with fields \texttt{get} and \texttt{putback}.

In the modules \texttt{Extraction.DesExtraction} and \texttt{.ConExtraction} lenses for  destructor extraction and the family of constructor extractions are provided. Implementing \texttt{get} and \texttt{putback} for destructor extraction was straightforward, in fact, we have already shown it in \autoref{sec:desextr}. Constructor extraction is more difficult, since it has a parameter, namely, the position of the constructor to be extracted. We have implemented this parameter as a list of non-negative integers representing the path to the position, and used this list in the two parts of the lens to find the correct variable or the correct constructor position, respectively.

The module \texttt{Unnest} implements the parametrized algorithm $\textsf{Unnest}_i$ as a function \texttt{unnestFor}, unnesting each function definition in turn. The function responsible for one function definition is basically implemented as a recursive loop over the coverage tree derived for the function definition, just as it is formally described, but using monadic computations, in order to not clutter the arguments of the functions that make up the implementation. We use a stack of monad transformers, with a \texttt{ReaderT} transformer for the typechecked program and \texttt{StateT} transformers for the coverage tree and the collected auxiliary function definitions. The destructor and constructor extractions used in the implementation are those provided by \texttt{Extraction.DesExtraction} and \texttt{Extraction.ConExtraction}. Coverage is derived once per function definition, before starting to unnest it; the functionality needed to derive the coverage and to deal with the coverage tree is provided by another module \texttt{CopatternCoverage} and its sub-module \texttt{CopatternCoverage.CCTree}. The unnesting stops when the coverage tree has depth zero or one. Function definitions which are already sufficiently unnested for the respective purpose $i$ are not unnested at all, especially, coverage isn't derived for these. The parameter of \texttt{unnestFor} is a predicate that determines whether a function definition is not yet sufficiently unnested for $i$, i.e., core defunctionalization or core refunctionalization.

The modules \texttt{Unnest.ForDefunc} and \texttt{Unnest.ForRefunc} provide predicates \texttt{isDNestedFunDef} and \texttt{isRNestedFunDef} that determine whether a function definition is not yet sufficiently unnested for core defunctionalization and core refunctionalization, respectively. Module \texttt{Unnest.ForDefunc} also provides a convenience function for unnesting for defunctionalization, \texttt{unnestForDefunc}, defined as \texttt{unnestFor isDNestedFunDef}; module \texttt{Unnest.ForRefunc} provides an analogous convenience function.

The modules \texttt{CoDataFragments.Defunc} and \texttt{CoDataFragments.Refunc} implement the defunctionalization of programs in the Codata Fragment of Uroboro and the refunctionalization of programs in the Data Fragment, respectively. Actually, the implementation extends the transformations to a slightly larger fragment; the Codata Fragment is extended by allowing constructors on the right-hand sides of equations, and the Data Fragment is extended by allowing destructors on the right-hand sides. The transformations just leave these unchanged. The benefit of this extension is that core defunctionalization and core refunctionalization, as provided by \texttt{CoreDR.CoreDefunc} and \texttt{CoreDR.Refunc}, can then be implemented straightforwardly: The function definitions are partitioned into those which are already defunctionalized (refunctionalized) and those which aren't, and the latter are then passed to the defunctionalization (refunctionalization) of \texttt{CoDataFragments.Defunc} ( \texttt{CoDataFragments.Refunc}).

Modules \texttt{Defunc} and \texttt{Refunc} implement the complete defunctionalization and refunctionalization algorithm, by subsequently using \texttt{Unnest.ForDefunc} (\texttt{Unnest.ForRefunc}) and \texttt{CoreDR.CoreDefunc} (\texttt{CoreDR.Refunc}). \texttt{Refunc} uses another step after unnesting and before core refunctionalization, which is to move the constructors to the first argument position, as described in \autoref{sssec:workaround}. This is implemented in module \texttt{CoreDR.MoveCon} using function \texttt{applyExtraction} from module \texttt{Extraction}.

The implementation doesn't yet deal correctly with empty (co)data definitions. This is because the representation of typechecked Uroboro programs we use doesn't provide the necessary functionality for this. In the result of typechecking (\texttt{Uroboro.Checker.Program}), it isn't possible to distinguish between positive and negative types. This can only be done indirectly via  constructor lists and destructor lists. But when these are empty, this isn't possible anymore. Consider the empty data type. We can't judge the type to be negative because it has no constructors, because that is wrong.\footnote{Cf. the age-old problem ``zero vs. nothing''.} But if we in general judge a type to be positive if it has no destructors, we run into the dual problem with the empty codata type. This makes it clear that knowing only constructor and destructor signatures doesn't suffice to determine which types are negative and which are positive. Instead, the declarations $\textbf{data } \sigma$ and $\textbf{codata } \sigma$ also need to be considered as signatures; the Uroboro library needs to be changed accordingly.

\section{Applications}
\label{sec:appl}

\subsection{The meta-circular interpreter}
\label{ssec:mci}

\begin{figure}
\begin{lstlisting}
data Nat where
  zero() : Nat
  succ(Nat) : Nat

data Exp where
  var(Nat) : Exp
  app(Exp, Exp) : Exp
  fun(Exp) : Exp

codata Val where
  Val.apply(Val) : Val

codata Env where
  Env.lookup(Nat) : Val

function nil() : Env where
  nil().lookup(x) = nil().lookup(x) -- may never be called

function cons(Val, Env) : Env where
  cons(val, env).lookup(zero()) = val
  cons(val, env).lookup(succ(n)) = env.lookup(n)

function eval(Exp, Env) : Val where
  eval(var(n), env) = env.lookup(n)
  eval(app(exp1, exp2), env) = eval(exp1, env).apply(eval(exp2, env))
  eval(fun(exp), env).apply(val) = eval(exp, cons(val, env))

function interpret(Exp) : Val where
  interpret(exp) = eval(exp, nil())
\end{lstlisting}
\caption{Meta-circular interpreter in Uroboro}
\label{fig:mci}
\end{figure}

\begin{figure}

\begin{lstlisting}
function eval(Exp, Env): Val where
  eval(fun(x0_0_0), x0_1) = autogen1_aux(x0_0_0, x0_1)
  eval(app(x0_0, x0_1), x1) = apply(eval(x0_0, x1), eval(x0_1, x1))
  eval(var(x0_0), x1) = lookup(x1, x0_0)

function autogen0_aux(Val, Env, Nat): Val where
  autogen0_aux(x0_0, x0_1, zero()) = x0_0
  autogen0_aux(x0_0, x0_1, succ(x1_0)) = lookup(x0_1, x1_0)

data Nat where
  zero(): Nat
  succ(Nat): Nat

data Exp where
  app(Exp, Exp): Exp
  fun(Exp): Exp
  var(Nat): Exp

function interpret(Exp): Val where
  interpret(exp) = eval(exp, nil())

data Val where
  autogen1_aux(Exp, Env): Val

data Env where
  cons(Val, Env): Env
  nil(): Env

function apply(Val, Val): Val where
  apply(autogen1_aux(x0_0_0, x0_1), x1) = eval(x0_0_0, cons(x1, x0_1))

function lookup(Env, Nat): Val where
  lookup(cons(x0_0, x0_1), x1) = autogen0_aux(x0_0, x0_1, x1)
  lookup(nil(), x) = lookup(nil(), x)
\end{lstlisting}
\caption{Meta-circular interpreter, defunctionalized}
\label{fig:mcidefunced}
\end{figure}

Reynolds' classical example for defunctionalization is the ``meta-circular'' interpreter for the lambda calculus\citep{reynolds72definitional}: Instead of writing an interpreter in a first-order language from scratch, he first straightforwardly translates the intuitive understanding of the semantics of the lambda calculus into a higher-order program, then mechanically defunctionalizes this program.

\citet{rendel15automatic} have already shown how Uroboro facilitates both the defunctionalization direction and the refunctionalization direction; they have also done so specifically for the meta-circular interpreter example. But their approach still required manual, even though mechanical, work to bring arbitrary Uroboro programs into either the form required by their defunctionalization or their refunctionalization, i.e., into the Codata or Data Fragment, respectively. Our work fully automatizes these pretransformations; all in all, we present an algorithm to defunctionalize any Uroboro program with copattern coverage, and one that refunctionalizes any such program. An implementation of our transformations, for instance, the one we provide in our Haskell library \texttt{uroboro-transformations}, can now be used in an IDE such that a programmer can start with an easier to understand higher-order program, like \citet{reynolds72definitional} did with the meta-circular interpreter, and then literally ``press a button'' to get the corresponding first-order program. This higher-order program can use the full expressiveness of Uroboro, not just that of either the Data or Codata Fragment. To illustrate this, in \autoref{fig:mci}, we give an Uroboro definition of \citeauthor{reynolds72definitional}' meta-circular interpreter that we think is as straightforward as one might wish for.\footnote{Except for the way environments are defined and used; this is due to Uroboro not supporting parametric polymorphism. We get back to this point in \autoref{sec:reluro}.} By this we mean that anybody with an intuitive understanding of lambda calculus (and the knowledge on how to read functional programs with data and codata types) should be able to comprehend it without thinking deeply about it (one shouldn't need to ``run an interpreter'' in their mind).

Note especially the equation

\begin{lstlisting}
  eval(fun(exp), env).apply(val) = eval(exp, cons(val, env)),
\end{lstlisting}

which is allowed neither in the Data nor in the Codata Fragment, since it has both a constructor and a destructor in its lhs. This is a point where the full expressiveness of Uroboro is helpful, as it allows to directly express the intuitive fact that, in the lambda calculus, abstractions evaluate to functions which can then be applied to another value $v$ by evaluating the body of the abstraction in a changed environment which binds $v$ to a variable.\footnote{As stated before, in this example, environments aren't as easily understood as the rest, due to the (current) limitations of Uroboro. The interpreter relies upon a naming scheme for variables that isn't immediately obvious, in order to bind $v$ to the correct variable; specifically, the natural number corresponding to the variable must be the next free, i.e., not corresponding to any variable, in the environment. Ideally, we would like to define environments using higher-order concepts involving parametric polymorphism, which Uroboro doesn't support (yet).}

Automatically defunctionalizing the interpreter shown in \autoref{fig:mci} using the executable \texttt{defunc} provided in our Haskell package \texttt{uroboro-transformations} yields the program shown in \autoref{fig:mcidefunced}. (The main module for \texttt{defunc} basically consists of a call to \texttt{Defunc.defunc}; this function can also be used when implementing automatic defunctionalization in an IDE.) The auto-generated constructor \texttt{autogen1\_aux} for \texttt{Val}, which is now a data type, corresponds to what is called a \textit{closure}. Using \texttt{defunc}, we have been able to automatically transform the meta-circular interpreter to a program where all values are represented by closures, and where function abstractions are directly evaluated to the corresponding closure, as described by the equation

\begin{lstlisting}
  eval(fun(x0_0_0), x0_1) = autogen1_aux(x_0_0, x0_1).
\end{lstlisting}

\subsection{Recognizer for Dyck words}
\label{ssec:dyck}

\begin{figure}

\begin{lstlisting}
data Nat where
  zero(): Nat
  succ(Nat): Nat

data Bool where
  true(): Bool
  false(): Bool

data Parenthesis where
  l(): Parenthesis
  r(): Parenthesis

data Word where
  empty(): Word
  cons(Parenthesis, Word): Word

function run(Word, Nat): Bool where
  run(empty(), zero()) = true()
  run(empty(), succ(n)) = false()
  run(cons(l(), ps), c) = run(ps, succ(c))
  run(cons(r(), ps), zero()) = false()
  run(cons(r(), ps), succ(c)) = run(ps, c)

function recognize(Word): Bool where
  recognize(ps) = run(ps, zero())
\end{lstlisting}
\caption{Dyck word recognizer in Uroboro}
\label{fig:dyck}
\end{figure}

\begin{figure}
\begin{lstlisting}
function autogen1_aux(Word, Nat): Bool where
  autogen1_aux(x0_1, x1) = x1.autogen3_aux(x0_1)

function recognize(Word): Bool where
  recognize(ps) = ps.run(zero())

codata Nat where
  Nat.autogen3_aux(Word): Bool
  Nat.autogen0_aux(): Bool

codata Parenthesis where
  Parenthesis.autogen2_aux(Word, Nat): Bool

codata Word where
  Word.run(Nat): Bool

function zero(): Nat where      -- A
  zero().autogen3_aux(x0_1) = false()
  zero().autogen0_aux() = true()

function succ(Nat): Nat where      -- B
  succ(x1_0).autogen3_aux(x0_1) = x0_1.run(x1_0)
  succ(x1_0).autogen0_aux() = false()

function l(): Parenthesis where
  l().autogen2_aux(x0_1, x1) = x0_1.run(succ(x1))

function r(): Parenthesis where
  r().autogen2_aux(x0_1, x1) = autogen1_aux(x0_1, x1)

function empty(): Word where
  empty().run(x1) = x1.autogen0_aux()

function cons(Parenthesis, Word): Word where
  cons(x0_0, x0_1).run(x1) = x0_0.autogen2_aux(x0_1, x1)
\end{lstlisting}
\caption{Dyck word recognizer, refunctionalized}
\label{fig:dyckrefunced}
\end{figure}

\begin{figure}
\begin{lstlisting}
fun recognize_refunctionalized ps
  = let (* run : word * (word option -> bool) -> bool *)
       fun run ([], c) 
           = c NONE
         | run (L :: ps, c)
           = run (ps, fn NONE => false | SOME ps => run (ps, c))      (* B *)
         | run (R :: ps, c)
           = c (SOME ps)
    in run (ps, fn NONE => true | SOME ps => false)      (* A *)
    end
\end{lstlisting}
\caption{Refunctionalized Dyck word recognizer of Danvy and Millikin}
\label{fig:danvydyckrefunced}
\end{figure}

\citet{danvy09refunctionalization} present a number of examples for refunctionalization, one of which is a recognizer for Dyck words, which they present in ML; \autoref{fig:dyck} shows the recognizer translated to Uroboro. \citeauthor{danvy09refunctionalization} refunctionalize the recognizer with respect to the counter, which is the second argument of function \texttt{run}. Using the executable \texttt{refunc} provided in our Haskell package \texttt{uroboro-transformations} to automatically refunctionalize the recognizer yields the program shown in \autoref{fig:dyckrefunced}; we omit the empty codata type definition for \texttt{Bool} and the empty function definitions for \texttt{true()} and \texttt{false()}, for the following reason.

Our transformation refunctionalizes \textit{everything}, which is not what \citeauthor{danvy09refunctionalization} do; especially, we are not interested in how \texttt{Bool} is refunctionalized. It might be desirable to be able to detect which parts of a program can be de- or refunctionalized independently; in the present case, \texttt{Bool} clearly can be excluded from the refunctionalization, since its constructors don't appear in any patterns. Similarly, \texttt{Word} and \texttt{Parenthesis} have also been refunctionalized; excluding them from the refunctionalization is not as simple as with \texttt{Bool}, since their constructors do appear in patterns.

Leaving this problem aside, we concentrate on recognizing the similarity between our transformation result, and the result of \citeauthor{danvy09refunctionalization}'s manual refunctionalization, which they present in Figure 3 of their work; we repeat this figure in \autoref{fig:danvydyckrefunced}. \citeauthor{danvy09refunctionalization} refunctionalize the program with respect to the counter. For this reason, first-class function terms in their transformation result---which are always continuations---should correspond to terms of codata type \texttt{Nat} in our transformation result. We will now show that this is actually the case.

The arguments of their \texttt{run} function are words and functions from optional words  (i.e., they use ML's option type) to booleans. Where do we find this signature in our transformation result? Look at the signature of our \texttt{run}, which in the refunctionalized form of the program is a destructor of \texttt{Word}. It has one argument with type \texttt{Nat}. Let us look at the codata type definition for \texttt{Nat}: Two destructors are defined, both of which have result type \texttt{Bool}, and one of them has no arguments while the other has exactly one argument with type \texttt{Word}. The reason for there being two destructors instead of only one using an option type is that we didn't introduce any arguments with option types, in contrast to \citeauthor{danvy09refunctionalization}, who, during their manual preprocessing of the program, introduce such arguments in order to merge several functions.

Looking at both the signatures \texttt{Word.run(Nat): Bool} and the signatures of the two destructors \texttt{Nat.autogen3\_aux(Word): Bool} and \texttt{Nat.autogen0\_aux(): Bool} for \texttt{Nat}, we can thus see the similarity of our \texttt{Word.run(Nat): Bool} to their \texttt{run : ord * (word option -> bool) -> bool} in their types. This is reinforced by comparing the equations defined for the two destructors of \texttt{Nat} with the continuation that is passed to \texttt{run} in the second to last line (indicated with comment `A') in Figure 3 of \citet{danvy09refunctionalization}. When given \texttt{NONE}, the continuation's result is defined as \texttt{true}, when given \texttt{SOME ps}, it is defined as \texttt{false}. Since this continuation is used at the initial call to \texttt{run}, where the stack is empty, we look at our function definition for \texttt{zero()} (also indicated with comment `A'). Applying destructor \texttt{autogen0\_aux(): Bool} to \texttt{zero()}, which corresponds to passing \texttt{NONE} to the continuation in the initial call of \texttt{run}, yields \texttt{true()}, applying destructor \texttt{autogen3\_aux(Word): Bool} to \texttt{zero()}, which corresponds to passing \texttt{SOME ps} to the continuation in the initial call of \texttt{run}, yields \texttt{false()}. The other call to \texttt{run} in \citeauthor{danvy09refunctionalization}'s refunctionalized program likewise corresponds to our function definition for \texttt{succ(Nat)} (relevant lines indicated by `B').

All in all, it can be seen that refunctionalizing the counter as done manually by \citeauthor{danvy09refunctionalization} is essentially the same as the part of our automatic refunctionalizing concerning the type \texttt{Nat}. The differences between our refunctionalization result and theirs are due to the merging of functions they perform and our unrestricted refunctionalization of all types.
